<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Byteman 4.0.4 has been released</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/XTfLQ3WoaK0/byteman-404-has-been-released.html" /><category term="feed_group_name_byteman" scheme="searchisko:content:tags" /><category term="feed_name_byteman" scheme="searchisko:content:tags" /><author><name>Andrew Dinn</name></author><id>searchisko:content:id:jbossorg_blog-byteman_4_0_4_has_been_released</id><updated>2018-07-16T20:34:00Z</updated><published>2018-07-16T20:34:00Z</published><content type="html">Byteman 4.0.4 is now available from the &lt;a href="http://www.jboss.org/byteman/downloads"&gt;Byteman downloads page&lt;/a&gt; and from the &lt;a href="https://oss.sonatype.org/index.html#nexus-search;quick%7Ebyteman"&gt;Maven Central repository&lt;/a&gt;. It is the latest release for use on JDK9+ runtimes. It is also recommended as the preferred release for use on JDK8- runtimes.&lt;br /&gt;&lt;br /&gt;Byteman 4.0.3 updates the 4.0.3 release to ensure that it works&amp;nbsp; correctly on the latest jdk11 releases. Specifically, it ensures that Byteman is able to process class files with a JDK11 class file version (the previous release would run on jdk11 but could only classes whose bytecode file version was for jdk10 or lower. More details can be found in the &lt;a href="http://downloads.jboss.org/byteman/4.0.4/ReleaseNotes.txt"&gt;Release Notes&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/XTfLQ3WoaK0" height="1" width="1" alt=""/&gt;</content><summary>Byteman 4.0.4 is now available from the Byteman downloads page and from the Maven Central repository. It is the latest release for use on JDK9+ runtimes. It is also recommended as the preferred release for use on JDK8- runtimes. Byteman 4.0.3 updates the 4.0.3 release to ensure that it works  correctly on the latest jdk11 releases. Specifically, it ensures that Byteman is able to process class fil...</summary><dc:creator>Andrew Dinn</dc:creator><dc:date>2018-07-16T20:34:00Z</dc:date><feedburner:origLink>http://bytemanblog.blogspot.com/2018/07/byteman-404-has-been-released.html</feedburner:origLink></entry><entry><title>Red Hat Process Automation Manager v7.0</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/Rt6jvJVSi8E/red-hat-process-automation-manager-v70.html" /><category term="feed_group_name_jbpm" scheme="searchisko:content:tags" /><category term="feed_name_kverlaen" scheme="searchisko:content:tags" /><category term="jBPM" scheme="searchisko:content:tags" /><category term="jbpm7" scheme="searchisko:content:tags" /><category term="Process Automation Manager" scheme="searchisko:content:tags" /><category term="product" scheme="searchisko:content:tags" /><category term="release" scheme="searchisko:content:tags" /><author><name>Kris Verlaenen</name></author><id>searchisko:content:id:jbossorg_blog-red_hat_process_automation_manager_v7_0</id><updated>2018-07-16T15:38:32Z</updated><published>2018-07-16T15:38:00Z</published><content type="html">&lt;div dir="ltr" style="text-align: left;" trbidi="on"&gt;&lt;div style="text-align: justify;"&gt;jBPM is completely open-source and therefore most of my blogs are typically about the latest and greatest feature that was just introduced in the community.&amp;nbsp; However, Red Hat also offers a supported version, with the testing, certification, and maintenance releases necessary for enterprise production use (for a quick intro on potential differences, take for example a look &lt;a href="https://www.redhat.com/en/technologies/jboss-middleware/community-or-enterprise"&gt;here&lt;/a&gt;).&lt;/div&gt;&lt;div style="text-align: justify;"&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style="text-align: justify;"&gt;And recently, as announced in this &lt;a href="https://www.redhat.com/en/about/press-releases/red-hat-unveils-next-generation-process-automation-offering"&gt;press release&lt;/a&gt;, Red Hat unveiled &lt;b&gt;&lt;a href="https://www.redhat.com/en/technologies/jboss-middleware/process-automation-manager"&gt;&lt;i&gt;Red Hat Process Automation Manager 7&lt;/i&gt;&lt;/a&gt;.&lt;/b&gt;&amp;nbsp; The most obvious change you might notice immediately is that the product was renamed - formerly known as Red Hat JBoss BPM Suite.&amp;nbsp; Since jBPM has evolved beyond just BPM - with features such as decision management, case management and constraint solving closely integrated - it was time to also reflect that in the product naming.&amp;nbsp; Similarly, &lt;a href="https://www.redhat.com/en/technologies/jboss-middleware/decision-manager"&gt;&lt;i&gt;Red Hat Decision Manager 7&lt;/i&gt;&lt;/a&gt; was released a few months ago, focusing on the Drools and Optaplanner bits.&lt;/div&gt;&lt;div style="text-align: justify;"&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style="text-align: justify;"&gt;However, nothing changes structurally.&amp;nbsp; Red Hat Process Automation Manager is based on jBPM (to be more precise, it was based on the jBPM 7.7.0.Final release) and actually is a super-set of Red Hat Decision Manager, so it also includes all the rules and constraint solving capabilities as well (Drools and Optaplanner).&amp;nbsp; Since it is completely open-source, you will see the same set of components there as you see in the community: the process execution server (kie-server), the web-based console (business-central aka the workbench - for both authoring and runtime deployment and administration), smart router, controller, Eclipse tooling, etc.&amp;nbsp; OpenShift images and templates (supporting these capabilities in the cloud) are available too for those targeting cloud deployment.&lt;/div&gt;&lt;div style="text-align: justify;"&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style="text-align: justify;"&gt;Red Hat Process Automation Manager also includes an advanced open source user experience platform from Red Hat partner &lt;i&gt;Entando&lt;/i&gt;. It can be used to quickly develop modern UI/UX layers for user interaction with business process applications, including a drag &amp;amp; drop UI development tool with widgets to create task lists, forms, process graphs, etc. &lt;/div&gt;&lt;div style="text-align: justify;"&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style="text-align: justify;"&gt;Red Hat Process Automation Manager is part of the &lt;a href="https://www.redhat.com/en/topics/automation/whats-business-automation"&gt;Business Automation portfolio&lt;/a&gt;, which includes Red Hat Process Automation Manager and Red Hat Decision Manager, but also the Red Hat Mobile Application Platform and in the future also big data analytics through Daikon.&lt;/div&gt;&lt;div style="text-align: justify;"&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style="text-align: justify;"&gt;More questions?&amp;nbsp; Take a look at the &lt;a href="https://www.redhat.com/en/technologies/jboss-middleware/process-automation-manager"&gt;product pages&lt;/a&gt; !&lt;/div&gt;&lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/Rt6jvJVSi8E" height="1" width="1" alt=""/&gt;</content><summary>jBPM is completely open-source and therefore most of my blogs are typically about the latest and greatest feature that was just introduced in the community.  However, Red Hat also offers a supported version, with the testing, certification, and maintenance releases necessary for enterprise production use (for a quick intro on potential differences, take for example a look here). And recently, as a...</summary><dc:creator>Kris Verlaenen</dc:creator><dc:date>2018-07-16T15:38:00Z</dc:date><feedburner:origLink>http://kverlaen.blogspot.com/2018/07/red-hat-process-automation-manager-v70.html</feedburner:origLink></entry><entry><title>Smart-Meter Data Processing Using Apache Kafka on OpenShift</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/-PJfr6kKpKI/" /><category term="AMQ" scheme="searchisko:content:tags" /><category term="Apache Kafka" scheme="searchisko:content:tags" /><category term="Big Data" scheme="searchisko:content:tags" /><category term="data streaming" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="Internet of Things" scheme="searchisko:content:tags" /><category term="jboss a-mq" scheme="searchisko:content:tags" /><category term="Kafka streams" scheme="searchisko:content:tags" /><category term="microservices" scheme="searchisko:content:tags" /><category term="OpenShift Container Platform" scheme="searchisko:content:tags" /><category term="Red Hat AMQ" scheme="searchisko:content:tags" /><category term="Red Hat OpenShift" scheme="searchisko:content:tags" /><category term="Red Hat OpenShift Container Platform" scheme="searchisko:content:tags" /><category term="Strimzi" scheme="searchisko:content:tags" /><author><name>Hugo Hiden</name></author><id>searchisko:content:id:jbossorg_blog-smart_meter_data_processing_using_apache_kafka_on_openshift</id><updated>2018-07-16T11:00:44Z</updated><published>2018-07-16T11:00:44Z</published><content type="html">&lt;p&gt;There is a major push in the United Kingdom to replace aging mechanical electricity meters with connected smart meters. New meters allow consumers to more closely monitor their energy usage and associated cost, and they enable the suppliers to automate the billing process because the meters automatically report fine-grained energy use.&lt;/p&gt; &lt;p&gt;This post describes an architecture for processing a stream of meter readings using &lt;a href="http://strimzi.io/"&gt;Strimzi&lt;/a&gt;, which offers support for running Apache Kafka in a container environment (&lt;a href="https://www.openshift.com/"&gt;Red Hat OpenShift&lt;/a&gt;). The data has been made available through a &lt;a href="http://www.networkrevolution.co.uk/"&gt;UK research project&lt;/a&gt; that collected data from energy producers, distributors, and consumers from 2011 to 2014. The TC1a dataset used here contains data from 8,000 domestic customers on half-hour intervals in the following form:&lt;/p&gt; &lt;p&gt;&lt;span id="more-506407"&gt;&lt;/span&gt;&lt;/p&gt; &lt;pre&gt;&lt;strong&gt; Location ID,Measurement Description,Parameter Type and Units,of capture,Parameter&lt;/strong&gt; 120,Electricity supply meter,Consumption in period [kWh],03/12/2011 00:00:00,0.067 120,Electricity supply meter,Consumption in period [kWh],03/12/2011 00:30:00,0.067 120,Electricity supply meter,Consumption in period [kWh],03/12/2011 01:00:00,0.066 120,Electricity supply meter,Consumption in period [kWh],03/12/2011 01:30:00,0.066&lt;/pre&gt; &lt;p&gt;As a single year of data represents approximately 25GB of comma-separated value (CSV) data, so importing and analyzing this data on a single machine is challenging. Also, when considering the relatively small number of customers monitored (8,000) in comparison with the number of actual customers served by any reasonably sized power company, the difficulties in processing this stream of data are magnified.&lt;/p&gt; &lt;p&gt;The approach adopted here is to process this data in the form of a stream of readings and make use of the &lt;a href="https://developers.redhat.com/blog/2018/05/07/announcing-amq-streams-apache-kafka-on-openshift/"&gt;Red Hat AMQ Streams&lt;/a&gt; distributed streaming platform to perform aggregations in real time as data is ingested into the application. The outputs of the system will be twofold: a dataset that can be used to train models of consumer use over a 24-hour period and a monitoring dashboard showing live demand levels.&lt;/p&gt; &lt;h2&gt;Components&lt;/h2&gt; &lt;p&gt;In order to architect a system that could scale to be deployed in a production environment, we adopted a microservices approach deployed on Red Hat OpenShift. The microservices are connected via Apache Kafka topics in the following pipeline:&lt;/p&gt; &lt;p&gt;&lt;img class=" aligncenter size-full wp-image-506427 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2018/07/architecture-1024x768.png" src="https://developers.redhat.com/blog/wp-content/uploads/2018/07/architecture.png" alt="" width="1024" height="768" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/07/architecture.png 1024w, https://developers.redhat.com/blog/wp-content/uploads/2018/07/architecture-300x225.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/07/architecture-768x576.png 768w" sizes="(max-width: 1024px) 100vw, 1024px" /&gt;&lt;br /&gt; Each of these blocks is deployed individually within OpenShift and makes use of an Apache Kafka Cluster provided by AMQ Streams.&lt;/p&gt; &lt;p&gt;The &lt;a href="https://github.com/cloudevents/spec"&gt;CloudEvents specification&lt;/a&gt; aims to enable portability of applications across multiple cloud providers. This is an ideal fit for describing the meter reading values that are sent from smart meters (or a simulator, in our case) to the energy supplier. Such portability will enable customers to change their electricity supplier without needing to change the hardware in their homes.&lt;/p&gt; &lt;h3&gt;REST Endpoint&lt;/h3&gt; &lt;p&gt;Data is ingested into the system via a Smart-Meter Simulator (which just reads the CSV data file) on a reading-by-reading basis. These meter readings are sent as CloudEvents to a RESTEasy microservice running as a Thorntail application. The application is deployed to OpenShift using the Fabric8 Maven plugin and converts the CloudEvents into Apache Kafka messages stored by AMQ Streams. This microservice makes use of the &lt;a href="https://github.com/aerogear/kafka-cdi"&gt;Kafka CDI library&lt;/a&gt;, which enables interaction with Apache Kafka topics via simple CDI annotations within Java code. This means that to connect to an Apache Kafka topic and send data to it, the amount of code is dramatically reduced to this:&lt;/p&gt; &lt;pre&gt;@ApplicationScoped @Path("/clnr") @KafkaConfig(bootstrapServers = "#{KAFKA_SERVICE_HOST}:#{KAFKA_SERVICE_PORT}") public class IngestAPI { private static final SimpleDateFormat format = new SimpleDateFormat("yyyy/MM/dd HH:mm:ss"); private final static Logger logger = Logger.getLogger(IngestAPI.class.getName()); @Producer private SimpleKafkaProducer&amp;#60;String, Reading&amp;#62; myproducer; &lt;/pre&gt; &lt;p&gt;The &lt;code&gt; @KafkaConfig &lt;/code&gt; annotation defines the location of the Apache Kafka cluster that will be used. This configuration can be entered directly or, if the data is formatted as &lt;code&gt; #{VALUE}&lt;/code&gt;, it will be taken from environment variables. This is useful when deploying the service into OpenShift as environment variables that are a key route for adding configuration data to deployments. The &lt;code&gt; @Producer &lt;/code&gt; annotation defines an output to an Apache Kafka topic where the code can send messages. Changes to the application configuration are handled transparently by OpenShift; a container is restarted if its configuration changes.&lt;/p&gt; &lt;p&gt;As part of any stream processing application, the correct consideration of timestamps within the data is essential. By default, the Apache Kafka Streams API adopts the system time (that is, wall clock time) when processing messages. So, if a message has been placed onto a topic without a timestamp in place, the default behavior will be to add a timestamp representing the current instance in time. Given that the data we are processing has been generated by other systems at a prior time to it being received, we need to configure Apache Kafka to use a timestamp that is embedded in the message payload.&lt;/p&gt; &lt;p&gt;There are two ways of achieving this:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Tell Apache Kafka to use a timestamp extractor class to explicitly pull a timestamp from the messages. Apache Kafka provides a mechanism for doing this when attaching the Streams API to a topic.&lt;/li&gt; &lt;li&gt;Add a timestamp to a message before it is placed onto a topic. This is the approach adopted in this example, primarily because the Kafka CDI library does not yet support the declaration of a timestamp extractor class in the streams annotation.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;To include a timestamp in a message, a new &lt;code&gt;ProducerRecord&lt;/code&gt; object must be created with the required metadata:&lt;/p&gt; &lt;pre&gt;Long timetamp = reading.getReadingTime(); String key = reading.getCustomerId(); ProducerRecord&amp;#60;String, Reading&amp;#62;; record = new ProducerRecord&amp;#60;&amp;#62;(“readings”, null, timestamp, key, reading); ((org.apache.kafka.clients.producer.Producer)myproducer).send(record); &lt;/pre&gt; &lt;p&gt;Here are some points to consider:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The timestamp and key are obtained directly from the parsed meter reading created when a row of data is posted into the ingest API.&lt;/li&gt; &lt;li&gt;It is necessary to explicitly cast the &lt;code&gt;SimpleKafkaProducer&lt;/code&gt; injected by the Kafka CDI library into an Apache Kafka &lt;code&gt;Producer&lt;/code&gt; object in order to be able to send a &lt;code&gt;ProducerRecord&lt;/code&gt; directly. When the Kafka CDI library is extended to include a timestamp extractor class annotation, this code to directly insert a timestamp would no longer be necessary.&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Current Demand-Level Calculator&lt;/h3&gt; &lt;p&gt;The purpose of the demand-level aggregator is to collect all of the readings in a one-hour period for all of the consumers in the dataset in order to provide an hour-by-hour demand level. In order to do this using the Apache Kafka Streams API, we need to aggregate the data into a rolling one-hour window. The code for this is shown below:&lt;/p&gt; &lt;pre&gt;@KafkaStream(input="ingest.api.out", output="demand.out") public KStream&amp;#60;String, JsonObject&amp;#62; demandStream(final KStream&amp;#60;String, JsonObject&amp;#62; source) { return source /*.peek((k, v)-&amp;#62;v.toString())*/ .selectKey((key, value) -&amp;#62; { return "ALL"; }).map((key, value) -&amp;#62; { MeterReading mr = new MeterReading(); mr.setCustomerId(value.getString("customerId")); mr.setTimestamp(value.getString("timestamp")); mr.setValue(value.getJsonNumber("kWh").doubleValue()); return new KeyValue&amp;#60;&amp;#62;(key, mr); }) .groupByKey(Serialized.with(new Serdes.StringSerde(), CafdiSerdes.Generic(MeterReading.class))) .windowedBy(TimeWindows.of(1 * 60 * 60 * 1000).until(1 * 60 * 60 * 1000)) .aggregate(() -&amp;#62; 0.0, (k, v, a) -&amp;#62; a + v.value, Materialized.&amp;#60;String, Double, WindowStore&amp;#60;Bytes, byte[]&amp;#62;&amp;#62;as("demand-store") .withValueSerde(Serdes.Double()) .withKeySerde(Serdes.String())) .toStream().map(new KeyValueMapper&amp;#60;Windowed&amp;#60;String&amp;#62;, Double, KeyValue&amp;#60;String, JsonObject&amp;#62;&amp;#62;() { @Override public KeyValue&amp;#60;String, JsonObject&amp;#62; apply(Windowed&amp;#60;String&amp;#62; key, Double value) { JsonObjectBuilder builder = Json.createObjectBuilder(); builder.add("timedate", format.format(new Date(key.window().start()))) .add("hour", hourFormat.format(new Date(key.window().start()))) .add("day", dayFormat.format(new Date(key.window().start()))) .add("timestamp", key.window().start()) .add("demand", value); return new KeyValue&amp;#60;&amp;#62;("DEMAND", builder.build()); } } ) &lt;/pre&gt; &lt;p&gt;In this code, there are a number of distinct steps to perform:&lt;/p&gt; &lt;p&gt;&lt;code&gt;.selectKey((key, value)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;When the data is ingested, the data in the input stream is keyed by customer ID; however, in this aggregation we want to group data from all users into a single set of time windows that represent the usage data for a particular hour. In order to do this, we apply a &lt;code&gt;selectKey&lt;/code&gt; operator that simply replaces the &lt;code&gt;customerId&lt;/code&gt; field with a default key of &lt;code&gt;“ALL”&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;&lt;code&gt;map((key, value)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The data passed between the various steps in this prototype is formatted as plain JSON as a lowest-common-denominator format. We are going to treat the data as a stream of &lt;code&gt;MeterReading objects&lt;/code&gt;, so this &lt;code&gt;map&lt;/code&gt; operator parses each record in the stream in turn and returns instances of the &lt;code&gt;MeterReading&lt;/code&gt; Java object.&lt;/p&gt; &lt;p&gt;&lt;code&gt;groupByKey&lt;/code&gt;&lt;/p&gt; &lt;p&gt;To perform any windowing operators, the Apache Kafka Streams API requires us to group the incoming data into a &lt;code&gt;KGroupedStream&lt;/code&gt;. Because we have already replaced the &lt;code&gt;customerId&lt;/code&gt; field with a standard key, this operator produces a grouped stream with a single group within it.&lt;/p&gt; &lt;p&gt;&lt;code&gt;.windowedBy(TimeWindows.of(1*60*60*1000).until(1*60*60*1000))&lt;/code&gt;&lt;/p&gt; &lt;p&gt;This is the operation that collects data from the input stream of meter readings into one-hour long windows of readings. In order to cater to some out-of-order events such as delayed readings, we retain this window of data for one hour after its time span. This allows us to insert events into this window up to one hour after they were expected to arrive. This parameter is clearly tunable and in a real system would be adjusted to balance between having results produced in an acceptable time span and potentially ignoring data that arrives too late.&lt;/p&gt; &lt;p&gt;&lt;code&gt;.aggregate(() -&amp;#62;0.0, (k, v, a) -&amp;#62; a + v.value&lt;/code&gt;&lt;/p&gt; &lt;p&gt;This step performs the actual work of adding up all of the meter readings in a single time window. The values are summed using an aggregator, which is initialized as a double-precision value of 0.0 and, when new readings fall within the window, this value is updated with the current meter reading value.&lt;/p&gt; &lt;p&gt;&lt;code&gt;public KeyValue&amp;#60;String,JsonObject&amp;#62; apply(Windowed&amp;#60;String&amp;#62;key,Double value)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Once we have a stream of total values for each time window, we apply another mapping function to these to add extra data such as timestamps, hour of data, and so on. This stream is then passed on to the downstream topics as JSON objects.&lt;/p&gt; &lt;h3&gt;Demand-Level Web Application&lt;/h3&gt; &lt;p&gt;Once a stream of hourly total demand level has been produced, the next step in the pipeline is to display this data via a simple visualization. Because the nature of this prototype is to investigate the feasibility of using Apache Kafka streams to process smart-meter data, the visualization in this demo is extremely simple in nature and just displays a bar chart that is updated when new hourly demand levels are available.&lt;/p&gt; &lt;p&gt;&lt;img class=" aligncenter size-full wp-image-506497 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2018/07/image-1024x399.png" src="https://developers.redhat.com/blog/wp-content/uploads/2018/07/image.png" alt="" width="2114" height="824" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/07/image.png 2114w, https://developers.redhat.com/blog/wp-content/uploads/2018/07/image-300x117.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/07/image-768x299.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2018/07/image-1024x399.png 1024w" sizes="(max-width: 2114px) 100vw, 2114px" /&gt;&lt;/p&gt; &lt;p&gt;In addition to the web chart, two other components are present in this application:&lt;/p&gt; &lt;h4&gt;Kafka CDI Topic Connection&lt;/h4&gt; &lt;p&gt;This component uses the simplest form of the Kafka CDI library to create a consumer method that receives demand-level JSON objects from the output of the demand-level calculator application:&lt;/p&gt; &lt;pre&gt;@ApplicationScoped @KafkaConfig(bootstrapServers = "#{KAFKA_SERVICE_HOST}:#{KAFKA_SERVICE_PORT}") public class DemandStreamListener { public DemandStreamListener() { System.out.println("Started demand stream listener"); } private static final Logger logger = Logger.getLogger(DemandStreamListener.class.getName()); @Consumer(topics = "demand.out", groupId = "1") public void onMessage(String key, JsonObject json){ logger.info(json.toString()); DemandWS.sendDemand(json); } &lt;/pre&gt; &lt;p&gt;The key part of this code is the &lt;code&gt;@Consumer&lt;/code&gt; annotation, which attaches to the Apache Kafka topic and marks the &lt;code&gt;onMessage&lt;/code&gt; method as the receiver for &lt;code&gt;JsonObject&lt;/code&gt; messages from the &lt;code&gt;demand.out&lt;/code&gt; Apache Kafka topic. As soon as messages are received, they are simply passed directly to the &lt;code&gt;DemandWS&lt;/code&gt; websocket.&lt;/p&gt; &lt;h4&gt;Websocket Handling&lt;/h4&gt; &lt;p&gt;In order to pass messages directly to the browser to support dynamic chart updating, we create a simple WebSocket endpoint that just keeps a list of connected WebSockets and passes demand level messages to each, in turn, every time a new message is received from the Kafka topic:&lt;/p&gt; &lt;pre&gt;@ServerEndpoint("/ws") @ApplicationScoped public class DemandWS { private static final Logger logger = Logger.getLogger(DemandWS.class.getName()); public static final Map&amp;#60;String, Session&amp;#62; clients = new ConcurrentHashMap&amp;#60;&amp;#62;(); @OnOpen public void socketOpened(Session client){ logger.info("Socked Opened: " + client.getId()); clients.put(client.getId(), client); } @OnClose public void socketClosed(Session client){ logger.info("Socket Closed: " + client.getId()); clients.remove(client.getId()); } public static void sendDemand(JsonObject demandMessage){ for(Session client : clients.values()){ client.getAsyncRemote().sendText(demandMessage.toString()); } } } &lt;/pre&gt; &lt;p&gt;The WebSocket implementation is extremely simple: it uses the &lt;code&gt;@ServiveEndpoint&lt;/code&gt; annotation to mark it as a WebSocket endpoint. This then allows clients to connect and receive real-time updates. The implementation tracks the lifecycle of the client connections so that messages can be routed to the currently active clients when new demand-level Apache Kafka messages are received.&lt;/p&gt; &lt;h3&gt;Customer Profile Aggregator&lt;/h3&gt; &lt;p&gt;The aim of the Customer Profile Aggregator microservice is to construct average usage patterns across the day for each customer. This is divided into 24-hour values representing the average amount of energy that a customer uses in that hour. These values can be combined with other datasets and used in machine learning applications, for example, to predict social class or future usage patterns for capacity planning.&lt;/p&gt; &lt;p&gt;As with the microservice that calculates the current demand, the profile aggregator makes use of the Apache Kafka Streams API to process the data.&lt;/p&gt; &lt;pre&gt;source.selectKey((key, value) -&amp;#62; value.getString("customerId")) .groupByKey(Serialized.with(new Serdes.StringSerde(), new MeterReadingSerializer())) .aggregate(()-&amp;#62;new CustomerRecord(), (k, v, a)-&amp;#62; a.update(v), CafdiSerdes.Generic(CustomerRecord.class)) .toStream().map((k, v)-&amp;#62;{ String json = ""; try { json = mapper.writeValueAsString(v); } catch (Exception e){ e.printStackTrace(); } return new KeyValue&amp;#60;&amp;#62;(v.customerId, json); }); &lt;/pre&gt; &lt;p&gt;Processing the data using the Apache Kafka Streams API is straightforward. Initially, records are grouped by the ID of the customer they relate to. Following this, they are converted to a &lt;code&gt;CustomerRecord&lt;/code&gt; object. This object contains a bucket for each hour of the day holding the energy used in that hour. When the &lt;code&gt;CustomerRecord&lt;/code&gt; for each customer is updated with a new &lt;code&gt;MeterReading&lt;/code&gt;, the relevant bucket is updated. Finally, the output is written as a JSON-formatted stream of &lt;code&gt;CustomerRecords&lt;/code&gt;. These records are persisted to a database to allow them to be consumed by other applications that are not necessarily developed in a streaming-aware manner.&lt;/p&gt; &lt;p&gt;There is a download REST service that can query this database to give the current hourly usage profiles of all of the customers in the form of a CSV file that can be imported into spreadsheets for analysis or used for machine learning and customer classification.&lt;/p&gt; &lt;h3&gt;Packaging the Components&lt;/h3&gt; &lt;p&gt;Because we are running this stack within OpenShift and making use of the AMQ Streams–packaged Apache Kafka platform, there are some considerations to be made when packaging and deploying the various components. Where applicable, the components are packaged as microservices using WildFly Swarm with the microprofile features enabled. This is reflected in the &lt;code&gt;pom.xml&lt;/code&gt; dependencies on &lt;code&gt;org.wildfly.swarm:microprofile&lt;/code&gt; and &lt;code&gt;org.aerogear.kafka:kafka-cdi-extensions&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;These dependencies are sufficient to enable the CDI environment that manages the injection of Apache Kafka connections which are used to transport messages. One thing that this does not provide yet, however, is the deployment and management of the requisite Apache Kafka topics which need to be present for the Kafka CDI injection process to work. Fortunately, AMQ Streams provides a route for declaratively creating Apache Kafka topics via config maps, which are an OpenShift/Kubernetes mechanism for providing key:value configuration properties to deployed applications.&lt;/p&gt; &lt;p&gt;To create a new topic, the following &lt;code&gt;ConfigMap&lt;/code&gt; entries need to be created:&lt;/p&gt; &lt;pre&gt;apiVersion: v1 kind: ConfigMap metadata: name: demand.out labels: strimzi.io/kind: topic strimzi.io/cluster: my-cluster data: name: demand.out partitions: "2" replicas: "1" &lt;/pre&gt; &lt;p&gt;The critical sections of this map lie in the &lt;code&gt;labels&lt;/code&gt; attributes. The AMQ Streams operator continuously monitors the config maps within the environment, and when maps containing the &lt;code&gt;strimzi.io&lt;/code&gt; labels appear, the operator will take steps to manage any required Apache KSt topics. The data section in the config map contains the actual details of the topics to be deployed.&lt;/p&gt; &lt;p&gt;Config maps like this are packaged as YAML files and deployed as part of the application using the Fabric8 Maven plugin. The Fabric8 plugin aims to simplify the deployment of microservices in both Kubernetes and OpenShift container environments and provides facilities for packaging up code and configuration into a single deployable unit. By convention, we packaged up the parts of this demo along with configurations for the output topics for each component. By doing this, once all components were deployed, we were left with a fully connected set of deployments exchanging Apache Kafka messages via topics to process the smart-meter data in real time as it was posted to the ingest API endpoint.&lt;/p&gt; &lt;h2&gt;Conclusions&lt;/h2&gt; &lt;p&gt;Apache Kafka and the Streams API make it fairly easy to perform the kinds of real-time aggregations needed to process this type of data stream. The operators provided for windowing and grouping coupled with the opportunities to add custom aggregators, mapping functions, and so on make for a rich set of stream-processing capabilities.&lt;/p&gt; &lt;p&gt;The Strimzi distribution of Apache Kafka makes it straightforward to deploy a tested, performant Apache Kafka cluster within a container environment with very little configuration effort. This allowed us to focus on the actual stream operators required to process the data&lt;/p&gt; &lt;p&gt;Running the demo in OpenShift provided an environment that allowed us to scale the various parts of the infrastructure to deal with increased volumes of meter readings. By keying the raw reading stream by customer ID we could, if needed, scale the aggregation of individual customer profiles over multiple calculation nodes.&lt;/p&gt; &lt;p&gt;The complex task of deploying all of the various components was greatly simplified through the use of the Fabric8 Maven plugin, which allowed us to package up code, configuration, and Apache Kafka topic definitions into deployable units and deploy these directly into OpenShift with a single command.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F07%2F16%2Fsmart-meter-streams-kafka-openshift%2F&amp;#38;linkname=Smart-Meter%20Data%20Processing%20Using%20Apache%20Kafka%20on%20OpenShift" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F07%2F16%2Fsmart-meter-streams-kafka-openshift%2F&amp;#38;linkname=Smart-Meter%20Data%20Processing%20Using%20Apache%20Kafka%20on%20OpenShift" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_google_plus" href="https://www.addtoany.com/add_to/google_plus?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F07%2F16%2Fsmart-meter-streams-kafka-openshift%2F&amp;#38;linkname=Smart-Meter%20Data%20Processing%20Using%20Apache%20Kafka%20on%20OpenShift" title="Google+" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F07%2F16%2Fsmart-meter-streams-kafka-openshift%2F&amp;#38;linkname=Smart-Meter%20Data%20Processing%20Using%20Apache%20Kafka%20on%20OpenShift" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F07%2F16%2Fsmart-meter-streams-kafka-openshift%2F&amp;#38;linkname=Smart-Meter%20Data%20Processing%20Using%20Apache%20Kafka%20on%20OpenShift" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F07%2F16%2Fsmart-meter-streams-kafka-openshift%2F&amp;#38;linkname=Smart-Meter%20Data%20Processing%20Using%20Apache%20Kafka%20on%20OpenShift" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F07%2F16%2Fsmart-meter-streams-kafka-openshift%2F&amp;#38;linkname=Smart-Meter%20Data%20Processing%20Using%20Apache%20Kafka%20on%20OpenShift" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F07%2F16%2Fsmart-meter-streams-kafka-openshift%2F&amp;#38;linkname=Smart-Meter%20Data%20Processing%20Using%20Apache%20Kafka%20on%20OpenShift" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F07%2F16%2Fsmart-meter-streams-kafka-openshift%2F&amp;#38;title=Smart-Meter%20Data%20Processing%20Using%20Apache%20Kafka%20on%20OpenShift" data-a2a-url="https://developers.redhat.com/blog/2018/07/16/smart-meter-streams-kafka-openshift/" data-a2a-title="Smart-Meter Data Processing Using Apache Kafka on OpenShift"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2018/07/16/smart-meter-streams-kafka-openshift/"&gt;Smart-Meter Data Processing Using Apache Kafka on OpenShift&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;RHD Blog&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/-PJfr6kKpKI" height="1" width="1" alt=""/&gt;</content><summary>There is a major push in the United Kingdom to replace aging mechanical electricity meters with connected smart meters. New meters allow consumers to more closely monitor their energy usage and associated cost, and they enable the suppliers to automate the billing process because the meters automatically report fine-grained energy use. This post describes an architecture for processing a stream of...</summary><dc:creator>Hugo Hiden</dc:creator><dc:date>2018-07-16T11:00:44Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2018/07/16/smart-meter-streams-kafka-openshift/</feedburner:origLink></entry><entry><title>Infinispan 9.3.1.Final and 9.4.0.Alpha1</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/0aeZkHcEoQ8/infinispan-931final-and-940alpha1.html" /><category term="feed_group_name_infinispan" scheme="searchisko:content:tags" /><category term="feed_name_infinispan" scheme="searchisko:content:tags" /><author><name>Dan Berindei</name></author><id>searchisko:content:id:jbossorg_blog-infinispan_9_3_1_final_and_9_4_0_alpha1</id><updated>2018-07-16T09:41:40Z</updated><published>2018-07-16T09:41:00Z</published><content type="html">We have 2 new releases to announce today:&lt;br /&gt;&lt;br /&gt;&lt;b&gt;9.3.1.Final&lt;/b&gt; includes some important bug fixes, and we recommend all users of 9.3.0.Final to upgrade:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;Fix for CVE-2018-1131 that allows unchecked deserialization in the server from binary java , XML and JSON payloads&lt;/li&gt;&lt;li&gt;Fixed transcoding from JSON/XML to java objects with deployed entities (&lt;a href="https://issues.jboss.org/browse/ISPN-9336"&gt;ISPN-9336&lt;/a&gt;)&lt;/li&gt;&lt;li&gt;Look up key in cache loader if the entry has expired but hasn't yet been removed from the data container (&lt;a href="https://issues.jboss.org/browse/ISPN-9370"&gt;ISPN-9370&lt;/a&gt;)&lt;/li&gt;&lt;li&gt;Avoid circular references in exceptions, as they were causing stack overflows with logback 1.2.x (&lt;a href="https://issues.jboss.org/browse/ISPN-9362"&gt;ISPN-9362&lt;/a&gt;) &lt;/li&gt;&lt;/ul&gt;See the full list of bug fixes &lt;a href="https://issues.jboss.org/secure/ReleaseNote.jspa?projectId=12310799&amp;amp;version=12338251"&gt;here&lt;/a&gt;.&amp;nbsp;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;b&gt;9.4.0.Alpha1&lt;/b&gt; its the first iteration towards our next big release. Highlights include:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;The Spring Cache provider now supports two configuration properties with which you can determine how long to wait for read and write operations respectively (&lt;a href="https://issues.jboss.org/browse/ISPN-9301"&gt;ISPN-9301&lt;/a&gt;).&lt;/li&gt;&lt;li&gt;You can now obtain nanosecond-resolution statistics for average read/write/remove time (&lt;a href="https://issues.jboss.org/browse/ISPN-9352"&gt;ISPN-9352&lt;/a&gt;).&lt;/li&gt;&lt;li&gt;Queries now throw an AvailabilityException if the cache is in degraded mode and partition mode isn’t ALLOW_READ_WRITES ([&lt;a href="https://issues.jboss.org/browse/ISPN-9340"&gt;ISPN-9340&lt;/a&gt;)&lt;/li&gt;&lt;li&gt;Admin Console: You can now delete cache from Administration console (&lt;a href="https://issues.jboss.org/browse/ISPN-7291"&gt;ISPN-7291&lt;/a&gt;).&lt;/li&gt;&lt;li&gt;Following up on the segmented data container in 9.3.0.Final, cache stores can now be segmented as well, allowing for better performance for bulk operations (ie. &lt;span style="font-family: &amp;quot;Courier New&amp;quot;, Courier, monospace;"&gt;cache.size()&lt;/span&gt;, &lt;span style="font-family: &amp;quot;Courier New&amp;quot;, Courier, monospace;"&gt;cache.entrySet().stream()&lt;/span&gt;)&lt;/li&gt;&lt;li&gt;The server-side Hot Rod parser is now generated automatically (&lt;a href="https://issues.jboss.org/browse/ISPN-8981"&gt;ISPN-8981&lt;/a&gt;)&amp;nbsp; &lt;/li&gt;&lt;/ul&gt;The full list of 9.4.0.Alpha1 fixes is &lt;a href="https://issues.jboss.org/secure/ReleaseNote.jspa?projectId=12310799&amp;amp;version=12337824"&gt;here&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;You can find both releases on our &lt;a href="https://infinispan.org/download/" target="_blank"&gt;download page&lt;/a&gt;. Please report any issue&lt;span style="font-size: small;"&gt;s in our &lt;a href="https://issues.jboss.org/projects/ISPN"&gt;issue tracker&lt;/a&gt; and &lt;/span&gt;&lt;span style="font-size: small;"&gt;join the conversation in our &lt;a href="https://infinispan.zulipchat.com/" target="_blank"&gt;Zulip Chat&lt;/a&gt; to shape up our next release.&lt;/span&gt;&lt;br /&gt;&lt;span style="font-size: small;"&gt;&amp;nbsp;&lt;/span&gt; &lt;span style="font-size: small;"&gt;&lt;span style="font-family: inherit;"&gt;&lt;/span&gt;&lt;/span&gt;&lt;img src="http://feeds.feedburner.com/~r/Infinispan/~4/DOUmfRw0nig" height="1" width="1" alt=""/&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/0aeZkHcEoQ8" height="1" width="1" alt=""/&gt;</content><summary>We have 2 new releases to announce today: 9.3.1.Final includes some important bug fixes, and we recommend all users of 9.3.0.Final to upgrade: Fix for CVE-2018-1131 that allows unchecked deserialization in the server from binary java , XML and JSON payloads Fixed transcoding from JSON/XML to java objects with deployed entities (ISPN-9336) Look up key in cache loader if the entry has expired but ha...</summary><dc:creator>Dan Berindei</dc:creator><dc:date>2018-07-16T09:41:00Z</dc:date><feedburner:origLink>http://feedproxy.google.com/~r/Infinispan/~3/DOUmfRw0nig/infinispan-931final-and-940alpha1.html</feedburner:origLink></entry><entry><title>Maciej Swiderski is the new jBPM community lead</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/QrmKtejHDPk/maciej-swiderski-is-new-jbpm-community.html" /><category term="feed_group_name_jbpm" scheme="searchisko:content:tags" /><category term="feed_name_kverlaen" scheme="searchisko:content:tags" /><category term="jBPM" scheme="searchisko:content:tags" /><author><name>Kris Verlaenen</name></author><id>searchisko:content:id:jbossorg_blog-maciej_swiderski_is_the_new_jbpm_community_lead</id><updated>2018-07-13T12:56:57Z</updated><published>2018-07-13T12:56:00Z</published><content type="html">&lt;div dir="ltr" style="text-align: left;" trbidi="on"&gt;&lt;div style="text-align: justify;"&gt;&lt;/div&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;/div&gt;&lt;div style="margin-left: 1em; margin-right: 1em;"&gt;&lt;/div&gt;&lt;br /&gt;&lt;div style="-webkit-text-stroke-width: 0px; background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small; font-style: normal; font-variant-caps: normal; font-variant-ligatures: normal; font-weight: 400; letter-spacing: normal; text-align: justify; text-decoration-color: initial; text-decoration-style: initial; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;"&gt;&lt;div style="-webkit-text-stroke-width: 0px; background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small; font-style: normal; font-variant-caps: normal; font-variant-ligatures: normal; font-weight: 400; letter-spacing: normal; text-decoration-color: initial; text-decoration-style: initial; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;"&gt;I am very glad to be able to announce that Maciej (aka "Magic") Swiderski will officially become the new jBPM community lead.&lt;/div&gt;&lt;div style="-webkit-text-stroke-width: 0px; background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small; font-style: normal; font-variant-caps: normal; font-variant-ligatures: normal; font-weight: 400; letter-spacing: normal; text-decoration-color: initial; text-decoration-style: initial; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;"&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style="-webkit-text-stroke-width: 0px; background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small; font-style: normal; font-variant-caps: normal; font-variant-ligatures: normal; font-weight: 400; letter-spacing: normal; text-decoration-color: initial; text-decoration-style: initial; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;"&gt;Maciej is one of the most productive engineers I have ever known.&amp;nbsp; And while that has led to huge expectations whenever he starts working on something new, he somehow manages to constantly over-deliver anyway.&amp;nbsp; To be fair, I have to say "officially" as he's been doing the bulk of that work for a long time.&amp;nbsp; Everyone that ever interacted in the community no doubt knows him, and his blog might be even more famous, probably almost any customer question is answered in one of the numerous blogs he has written over the last few years.&amp;nbsp; I remember exchanging emails with him in 2010, the early days of jBPM 5, but he was even active in the community before that.&amp;nbsp; He joined full-time a few years later, and ever since has taken care of anything related to process execution for years.&amp;nbsp; Nowadays, he's involved in so much (from case management to our cloud story) and producing so much work that I saw no other solution than to just make him responsible for it ;-)&lt;/div&gt;&lt;div style="-webkit-text-stroke-width: 0px; background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small; font-style: normal; font-variant-caps: normal; font-variant-ligatures: normal; font-weight: 400; letter-spacing: normal; text-decoration-color: initial; text-decoration-style: initial; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;"&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://avatars3.githubusercontent.com/u/904474?s=460&amp;amp;v=4" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img alt="Afbeeldingsresultaat voor maciej swiderski" border="0" class="irc_mi" height="200" src="https://avatars3.githubusercontent.com/u/904474?s=460&amp;amp;v=4" style="margin-top: 94px;" width="200" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style="-webkit-text-stroke-width: 0px; background-color: white; color: #222222; font-family: Arial, Helvetica, sans-serif; font-size: small; font-style: normal; font-variant-caps: normal; font-variant-ligatures: normal; font-weight: 400; letter-spacing: normal; text-decoration-color: initial; text-decoration-style: initial; text-indent: 0px; text-transform: none; white-space: normal; word-spacing: 0px;"&gt;Well deserved, and long overdue !&amp;nbsp; Congratulations Maciej.&lt;br /&gt;&lt;br /&gt;PS: I'm not going anywhere in case anyone is wondering, still 100% involved, but given Maciej's continuous focus on the community and with the team growing this is the right move !&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/QrmKtejHDPk" height="1" width="1" alt=""/&gt;</content><summary>I am very glad to be able to announce that Maciej (aka "Magic") Swiderski will officially become the new jBPM community lead. Maciej is one of the most productive engineers I have ever known.  And while that has led to huge expectations whenever he starts working on something new, he somehow manages to constantly over-deliver anyway.  To be fair, I have to say "officially" as he's been doing the b...</summary><dc:creator>Kris Verlaenen</dc:creator><dc:date>2018-07-13T12:56:00Z</dc:date><feedburner:origLink>http://kverlaen.blogspot.com/2018/07/maciej-swiderski-is-new-jbpm-community.html</feedburner:origLink></entry><entry><title>This week in JBoss, 13th of July 2018 - Release It!</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/Vd5pMtqa094/this-week-in-jboss-13th-of-july-2018-release-it" /><category term="Apicurio" scheme="searchisko:content:tags" /><category term="Apiman" scheme="searchisko:content:tags" /><category term="byteman" scheme="searchisko:content:tags" /><category term="debezium" scheme="searchisko:content:tags" /><category term="feed_group_name_global" scheme="searchisko:content:tags" /><category term="feed_name_weeklyeditorial" scheme="searchisko:content:tags" /><category term="fuse;" scheme="searchisko:content:tags" /><category term="hibernate ogm" scheme="searchisko:content:tags" /><category term="hibernate-orm" scheme="searchisko:content:tags" /><category term="hibernate;" scheme="searchisko:content:tags" /><category term="infinispan;" scheme="searchisko:content:tags" /><category term="jbossts" scheme="searchisko:content:tags" /><category term="thorntail" scheme="searchisko:content:tags" /><category term="transactions" scheme="searchisko:content:tags" /><category term="wildfly swarm" scheme="searchisko:content:tags" /><author><name>Mark Little</name></author><id>searchisko:content:id:jbossorg_blog-this_week_in_jboss_13th_of_july_2018_release_it</id><updated>2018-07-13T12:08:13Z</updated><published>2018-07-13T12:08:00Z</published><content type="html">&lt;!-- [DocumentBodyStart:e1dfb38f-5751-40ad-b230-501ad5f99183] --&gt;&lt;div class="jive-rendered-content"&gt;&lt;p&gt;This editorial looks like it's going to be about release after release after release! The teams have really been busy!&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;First Marc has written a few articles about the new APIMan release (&lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/version_1_4_of_apiman_is_released" rel="nofollow"&gt;version 1.4&lt;/a&gt;) followed quickly by &lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/version_1_4_3_of_apiman_is_released" rel="nofollow"&gt;1.4.3&lt;/a&gt; (if anyone finds 1.4.1 and 1.4.2 please return them to Marc care of JBoss!) He's also found the time to write about how you can &lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/customising_path_patterns_for_your_apiman_gateway" rel="nofollow"&gt;customise your path patterns for the gateway&lt;/a&gt;.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;While we're at it, of course let's not forget about the equally interesting &lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/hibernate_community_newsletter_12_2018" rel="nofollow"&gt;Hibernate Community Newsletter&lt;/a&gt;. And on the theme of Hibernate, Hibernate Search 5.10 has &lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/second_maintenance_release_for_hibernate_search_5_10" rel="nofollow"&gt;another maintenance release&lt;/a&gt; and &lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/hibernate_orm_5_1_15_final_released" rel="nofollow"&gt;Hibernate ORM 5.1.15.Final&lt;/a&gt; along with &lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/hibernate_orm_5_3_2_final_released" rel="nofollow"&gt;ORM 5.3.2.Final&lt;/a&gt; were also released by the team, followed closely by &lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/hibernate_ogm_5_4_0_beta2_release" rel="nofollow"&gt;OGM 5.4.0.Beta2&lt;/a&gt;.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;Other releases over the period include &lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/infinispan_9_3_0_final_is_out" rel="nofollow"&gt;Infinispan 9.3.0.Final&lt;/a&gt;, &lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/announcing_thorntail_2_0_0_final" rel="nofollow"&gt;Thorntail 2.0.0.Final&lt;/a&gt;, a couple of &lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/resteasy_3_6_0_final_and_4_0_0_beta4" rel="nofollow"&gt;RESTeasy releases&lt;/a&gt;, &lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/byteman_4_0_3_has_been_released" rel="nofollow"&gt;ByteMan 4.0.3&lt;/a&gt;, the ever popular Debezium &lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/debezium_0_8_0_cr1_is_released" rel="nofollow"&gt;had a release&lt;/a&gt;, &lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/apache_camel_2_22_released_with_spring_boot_2_support" rel="nofollow"&gt;Apache Camel 2.22&lt;/a&gt; came out, and &lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/keycloak_4_1_0_final_released" rel="nofollow"&gt;Keycloak 4.1.0.Final&lt;/a&gt; (check out this article around &lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/keycloak_on_kubernetes" rel="nofollow"&gt;Keycloak on Kubernetes&lt;/a&gt; too!)&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;Not quite a release but definitely newsworthy, &lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/mario_fusco_is_the_new_drools_project_lead" rel="nofollow"&gt;Mario Fusco is the new Drools lead&lt;/a&gt;! Well done Mario!!&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;Chritina Lin has written a &lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/fuse_contract_first_api_design_with_apicurio_and_fuse_camel_part_one" rel="nofollow"&gt;great first article&lt;/a&gt; on contract first design with Apicurio and Fuse.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;Let's wrap up with a personal favourite: the JBossTS team have done some interesting work around the LRCO optimisation and you can read about it in &lt;a class="jive-link-external-small" href="https://planet.jboss.org/post/narayana_commit_markable_resource_a_faultless_lrco_for_jdbc_datasources" rel="nofollow"&gt;this article&lt;/a&gt;.&lt;/p&gt;&lt;p style="min-height: 8pt; padding: 0px;"&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;OK that's it for now. Enjoy!&lt;/p&gt;&lt;/div&gt;&lt;!-- [DocumentBodyEnd:e1dfb38f-5751-40ad-b230-501ad5f99183] --&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/Vd5pMtqa094" height="1" width="1" alt=""/&gt;</content><summary>This editorial looks like it's going to be about release after release after release! The teams have really been busy!   First Marc has written a few articles about the new APIMan release (version 1.4) followed quickly by 1.4.3 (if anyone finds 1.4.1 and 1.4.2 please return them to Marc care of JBoss!) He's also found the time to write about how you can customise your path patterns for the gateway...</summary><dc:creator>Mark Little</dc:creator><dc:date>2018-07-13T12:08:00Z</dc:date><feedburner:origLink>https://developer.jboss.org/blogs/weekly-editorial/2018/07/13/this-week-in-jboss-13th-of-july-2018-release-it</feedburner:origLink></entry><entry><title>Contract-First API Design with Apicurio and Red Hat Fuse/Camel</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/C5nm9tnzf08/" /><category term="Agile Integration" scheme="searchisko:content:tags" /><category term="apache camel" scheme="searchisko:content:tags" /><category term="API" scheme="searchisko:content:tags" /><category term="Apicurio" scheme="searchisko:content:tags" /><category term="Camel" scheme="searchisko:content:tags" /><category term="contract first design" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="JBoss Fuse" scheme="searchisko:content:tags" /><category term="microservices" scheme="searchisko:content:tags" /><category term="Modern App Dev" scheme="searchisko:content:tags" /><category term="OpenAPI" scheme="searchisko:content:tags" /><category term="Red Hat Fuse" scheme="searchisko:content:tags" /><category term="Red Hat OpenShift Container Platform" scheme="searchisko:content:tags" /><author><name>Christina Lin</name></author><id>searchisko:content:id:jbossorg_blog-contract_first_api_design_with_apicurio_and_red_hat_fuse_camel</id><updated>2018-07-12T16:00:52Z</updated><published>2018-07-12T16:00:52Z</published><content type="html">&lt;p&gt;This is part one of my two-article series that demonstrates how to implement contract-first API design using &lt;a href="https://www.apicur.io/"&gt;Apicurio&lt;/a&gt; and &lt;a href="https://www.redhat.com/en/technologies/jboss-middleware/fuse"&gt;Red Hat Fuse&lt;/a&gt;.  It covers how to create an OpenAPI standard document as the contract between API providers and consumers using Apicurio Studio. It also shows how to quickly create mock tests using &lt;a href="https://developers.redhat.com/products/fuse/overview/"&gt;Red Hat Fuse&lt;/a&gt; which is based on Camel.&lt;/p&gt; &lt;p&gt;There are two common approaches when it comes to creating APIs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Code first (top-down)&lt;/li&gt; &lt;li&gt;Contract first (bottom-up)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;span id="more-505057"&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2&gt;Code-First Approach&lt;/h2&gt; &lt;p&gt;To ESB developers, these two approaches aren&amp;#8217;t new. Before, it was the WSDL that defined the contract of the service. We were doing a lot more coding first, because it&amp;#8217;s easy to write a couple of Java classes and generate the WSDL for the consumers. This is the &lt;em&gt;code-first&lt;/em&gt; approach, which has been the most common in the past.&lt;/p&gt; &lt;div class="separator"&gt;&lt;a href="https://4.bp.blogspot.com/-xn1vGjSAAnE/WzqGGNtRwcI/AAAAAAAAFkw/5QNFPyU4AJ4-8S8X8xjwHLL4aEjfA_B3QCLcBGAs/s1600/Screen%2BShot%2B2018-07-03%2Bat%2B4.07.01%2BAM.png"&gt;&lt;img class="aligncenter" src="https://4.bp.blogspot.com/-xn1vGjSAAnE/WzqGGNtRwcI/AAAAAAAAFkw/5QNFPyU4AJ4-8S8X8xjwHLL4aEjfA_B3QCLcBGAs/s400/Screen%2BShot%2B2018-07-03%2Bat%2B4.07.01%2BAM.png" width="400" height="270" border="0" /&gt;&lt;/a&gt;&lt;/div&gt; &lt;p&gt;Code-first development can be pretty straightforward if the consumer of your application has decided how they want the service to work. There is always a preliminary discussion between the developer and consumer about the data to be exchanged. It is likely that there is some notion of the &amp;#8220;contract&amp;#8221; for the service, but often it is implicit. With this approach small changes are inevitable, and it takes a toll on the developer to grind through the long process of making all these updates and getting everything right. Since the contract isn&amp;#8217;t explicitly spelled out, these changes might actually break things because the developer and the consumer each have different understanding of the service&amp;#8217;s intended operation.&lt;/p&gt; &lt;p&gt;.&lt;/p&gt; &lt;h2&gt;Contract-First Approach&lt;/h2&gt; &lt;p&gt;Business users and citizen users/developers can use the new OpenAPI specification to negotiate and perhaps perform a couple of pre-tests with the consumer before the design gets handed over to the developer. This design approach is called &lt;em&gt;contract first&lt;/em&gt;. It has become more and more popular because it prevents the developer from wasting time while negotiating how the service should be provided.&lt;/p&gt; &lt;div class="separator"&gt;&lt;a href="https://1.bp.blogspot.com/-yveltlUuzW4/WzqLuvxMhrI/AAAAAAAAFk8/SbjUNBZ3udIGlMkTSlITPhSqKUT3N6AHgCLcBGAs/s1600/Screen%2BShot%2B2018-07-03%2Bat%2B4.31.35%2BAM.png"&gt;&lt;img class="aligncenter" src="https://1.bp.blogspot.com/-yveltlUuzW4/WzqLuvxMhrI/AAAAAAAAFk8/SbjUNBZ3udIGlMkTSlITPhSqKUT3N6AHgCLcBGAs/s400/Screen%2BShot%2B2018-07-03%2Bat%2B4.31.35%2BAM.png" width="400" height="222" border="0" /&gt;&lt;/a&gt;&lt;/div&gt; &lt;p&gt;Obviously, there are many ways to implement a contract-first API. I am going to demonstrate how it can be done using Apicurio and Red Hat Fuse. I will be using Apicurio to define APIs and automatically generate the Red Hat Fuse project for the purpose of quick testing.&lt;/p&gt; &lt;h2&gt;Creating a Customer Service API&lt;/h2&gt; &lt;p&gt;In this example demo, we will be providing customer info to our consumer as a service. For the sake of this demo, we will start by retrieving and creating a customer service.&lt;/p&gt; &lt;div class="separator"&gt;&lt;a href="http://1.bp.blogspot.com/-XTDagvBT42k/Wztx7K9XbJI/AAAAAAAAFlM/JqqlHpz7vXA2rKSZZz5eOckSRnVq7NnWACK4BGAYYCw/s1600/Screen%2BShot%2B2018-07-03%2Bat%2B8.03.27%2BAM.png"&gt;&lt;img src="https://1.bp.blogspot.com/-XTDagvBT42k/Wztx7K9XbJI/AAAAAAAAFlM/JqqlHpz7vXA2rKSZZz5eOckSRnVq7NnWACK4BGAYYCw/s400/Screen%2BShot%2B2018-07-03%2Bat%2B8.03.27%2BAM.png" width="400" height="196" border="0" /&gt;&lt;/a&gt;&lt;/div&gt; &lt;p&gt;&amp;#160;&lt;/p&gt; &lt;h3&gt;Defining the Application with the OpenAPI Specification in Apicurio&lt;/h3&gt; &lt;p&gt;Apicurio is a web-based open source tool for designing APIs that are based on the OpenAPI specification.&lt;/p&gt; &lt;p&gt;If you don&amp;#8217;t already have an Apicurio account, you need to first register for one.&lt;/p&gt; &lt;div class="separator"&gt;&lt;a href="http://1.bp.blogspot.com/-ChQnfK46nzg/WztygyxYt2I/AAAAAAAAFlY/HU51E1gVIqoI1M2OhSgj30wRDV4SsU0BACK4BGAYYCw/s1600/Screen%2BShot%2B2018-07-03%2Bat%2B8.03.42%2BAM.png"&gt;&lt;img src="https://1.bp.blogspot.com/-ChQnfK46nzg/WztygyxYt2I/AAAAAAAAFlY/HU51E1gVIqoI1M2OhSgj30wRDV4SsU0BACK4BGAYYCw/s640/Screen%2BShot%2B2018-07-03%2Bat%2B8.03.42%2BAM.png" width="640" height="192" border="0" /&gt;&lt;/a&gt;&lt;/div&gt; &lt;p&gt;After registering, you will be redirected to the main screen of Apicurio.&lt;b&gt; &lt;/b&gt;Then, after discussing with the consumer what they wish to have for the customer service, you can start creating the contract by clicking &lt;b&gt;Create New API&lt;/b&gt;.&lt;br /&gt; &lt;b&gt;&lt;br /&gt; &lt;/b&gt;&lt;a href="http://2.bp.blogspot.com/-PwRajchevj0/WztyzBw4vQI/AAAAAAAAFlk/YafjXYkrJL0t46mWjImoox8IoAe37u2zACK4BGAYYCw/s1600/Screen%2BShot%2B2018-07-03%2Bat%2B8.06.13%2BAM.png"&gt;&lt;img src="https://2.bp.blogspot.com/-PwRajchevj0/WztyzBw4vQI/AAAAAAAAFlk/YafjXYkrJL0t46mWjImoox8IoAe37u2zACK4BGAYYCw/s640/Screen%2BShot%2B2018-07-03%2Bat%2B8.06.13%2BAM.png" width="640" height="332" border="0" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What you need to do next is pretty simple:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Create the API (service).&lt;/li&gt; &lt;li&gt;Create the data definitions (if any are required).&lt;/li&gt; &lt;li&gt;Add paths; define parameters and operations; and return responses to the paths.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Enter the name of the service, and it would be nice to add a description for the service so it&amp;#8217;s easier for people to understand what every path means.&lt;/p&gt; &lt;p&gt;&lt;a href="http://1.bp.blogspot.com/-nYGSbZRAjac/WzuWDhWRC8I/AAAAAAAAFpI/jzK4uncQ8FgXzCIj-K9Dc3oBgIhb8pRNgCK4BGAYYCw/s1600/Screen%2BShot%2B2018-07-03%2Bat%2B11.27.59%2BAM.png"&gt;&lt;img src="https://1.bp.blogspot.com/-nYGSbZRAjac/WzuWDhWRC8I/AAAAAAAAFpI/jzK4uncQ8FgXzCIj-K9Dc3oBgIhb8pRNgCK4BGAYYCw/s640/Screen%2BShot%2B2018-07-03%2Bat%2B11.27.59%2BAM.png" width="640" height="158" border="0" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;div&gt;Enter a customer definition to show info about what we are going exchange.&lt;/div&gt; &lt;p&gt;&lt;a href="http://3.bp.blogspot.com/-lYJT8fTF56E/Wzt220wWDjI/AAAAAAAAFmI/26TUka6CyPcZfen6tCUQX-Zu1zVuC911gCK4BGAYYCw/s1600/Screen%2BShot%2B2018-07-03%2Bat%2B8.11.25%2BAM.png"&gt;&lt;img src="https://3.bp.blogspot.com/-lYJT8fTF56E/Wzt220wWDjI/AAAAAAAAFmI/26TUka6CyPcZfen6tCUQX-Zu1zVuC911gCK4BGAYYCw/s640/Screen%2BShot%2B2018-07-03%2Bat%2B8.11.25%2BAM.png" width="640" height="216" border="0" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Add and define the properties and their data type.&lt;/p&gt; &lt;div class="separator"&gt;&lt;a href="http://1.bp.blogspot.com/-abCM_73sxto/Wzt4Kg7FnpI/AAAAAAAAFmU/qJbyWPUwf2sZKQfTYjPuI8p1aqXDGo3iwCK4BGAYYCw/s1600/Screen%2BShot%2B2018-07-03%2Bat%2B8.24.36%2BAM.png"&gt;&lt;img src="https://1.bp.blogspot.com/-abCM_73sxto/Wzt4Kg7FnpI/AAAAAAAAFmU/qJbyWPUwf2sZKQfTYjPuI8p1aqXDGo3iwCK4BGAYYCw/s640/Screen%2BShot%2B2018-07-03%2Bat%2B8.24.36%2BAM.png" width="640" height="178" border="0" /&gt;&lt;/a&gt;&lt;a href="http://4.bp.blogspot.com/-7KqBiY8QYP0/Wzt4MKqqdnI/AAAAAAAAFmc/78f-wIvgAuALhj2g5aFMyQ4gs0b02SE4QCK4BGAYYCw/s1600/Screen%2BShot%2B2018-07-03%2Bat%2B8.27.54%2BAM.png"&gt;&lt;img src="https://4.bp.blogspot.com/-7KqBiY8QYP0/Wzt4MKqqdnI/AAAAAAAAFmc/78f-wIvgAuALhj2g5aFMyQ4gs0b02SE4QCK4BGAYYCw/s400/Screen%2BShot%2B2018-07-03%2Bat%2B8.27.54%2BAM.png" width="400" height="251" border="0" /&gt;&lt;/a&gt;&lt;/div&gt; &lt;div class="separator"&gt;&lt;/div&gt; &lt;div&gt;&lt;/div&gt; &lt;div class=""&gt;Then you can start adding the paths to the document.&lt;/div&gt; &lt;div&gt;&lt;/div&gt; &lt;div&gt;&lt;/div&gt; &lt;div class="separator"&gt;&lt;a href="http://4.bp.blogspot.com/-swa98cIRZzs/Wzt_DIkuxMI/AAAAAAAAFms/0TjvMT6wqTI62IOrL8PayzaE9zFE3aMDQCK4BGAYYCw/s1600/Screen%2BShot%2B2018-07-03%2Bat%2B8.07.43%2BAM.png"&gt;&lt;img src="https://4.bp.blogspot.com/-swa98cIRZzs/Wzt_DIkuxMI/AAAAAAAAFms/0TjvMT6wqTI62IOrL8PayzaE9zFE3aMDQCK4BGAYYCw/s640/Screen%2BShot%2B2018-07-03%2Bat%2B8.07.43%2BAM.png" width="640" height="170" border="0" /&gt;&lt;/a&gt;&lt;/div&gt; &lt;div class="separator"&gt;&lt;/div&gt; &lt;div&gt;&lt;/div&gt; &lt;div class="separator"&gt;Add parameters and define their type.&lt;/div&gt; &lt;div&gt;&lt;/div&gt; &lt;div class="separator"&gt;&lt;/div&gt; &lt;div class="separator"&gt;&lt;a href="http://4.bp.blogspot.com/-OZ3nXxrFT6I/Wzt_PNEi4HI/AAAAAAAAFm0/AoMy4DeX7ec9PmkurBta2zzT-M7b4qIBwCK4BGAYYCw/s1600/Screen%2BShot%2B2018-07-03%2Bat%2B8.28.04%2BAM.png"&gt;&lt;img src="https://4.bp.blogspot.com/-OZ3nXxrFT6I/Wzt_PNEi4HI/AAAAAAAAFm0/AoMy4DeX7ec9PmkurBta2zzT-M7b4qIBwCK4BGAYYCw/s640/Screen%2BShot%2B2018-07-03%2Bat%2B8.28.04%2BAM.png" width="640" height="240" border="0" /&gt;&lt;/a&gt;&lt;/div&gt; &lt;p&gt;&amp;#160;&lt;/p&gt; &lt;div class="separator"&gt;&lt;/div&gt; &lt;p&gt;&lt;a href="http://1.bp.blogspot.com/-2x-JSSeBHj0/Wzt_mgy1fXI/AAAAAAAAFnE/B7PsCRbU3487BaAP6RnzfRJbPEgTSOHNgCK4BGAYYCw/s1600/Screen%2BShot%2B2018-07-03%2Bat%2B8.28.50%2BAM.png"&gt;&lt;img src="https://1.bp.blogspot.com/-2x-JSSeBHj0/Wzt_mgy1fXI/AAAAAAAAFnE/B7PsCRbU3487BaAP6RnzfRJbPEgTSOHNgCK4BGAYYCw/s640/Screen%2BShot%2B2018-07-03%2Bat%2B8.28.50%2BAM.png" width="640" height="402" border="0" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="http://4.bp.blogspot.com/-UaS3tO6kqpQ/Wzt_nSLFVHI/AAAAAAAAFnM/2p3CJNQpDRM7c6qHk0noKfcBgTj19F2hQCK4BGAYYCw/s1600/Screen%2BShot%2B2018-07-03%2Bat%2B8.28.58%2BAM.png"&gt;&lt;img src="https://4.bp.blogspot.com/-UaS3tO6kqpQ/Wzt_nSLFVHI/AAAAAAAAFnM/2p3CJNQpDRM7c6qHk0noKfcBgTj19F2hQCK4BGAYYCw/s640/Screen%2BShot%2B2018-07-03%2Bat%2B8.28.58%2BAM.png" width="640" height="236" border="0" /&gt;&lt;/a&gt;&lt;br /&gt; &lt;a href="http://3.bp.blogspot.com/-nj6qZhw1GBo/Wzt_tsuHMeI/AAAAAAAAFnU/3ZGagdOEu9Q50qJaxU1QFrKgpCLeX4kxwCK4BGAYYCw/s1600/Screen%2BShot%2B2018-07-03%2Bat%2B8.29.21%2BAM.png"&gt;&lt;img src="https://3.bp.blogspot.com/-nj6qZhw1GBo/Wzt_tsuHMeI/AAAAAAAAFnU/3ZGagdOEu9Q50qJaxU1QFrKgpCLeX4kxwCK4BGAYYCw/s640/Screen%2BShot%2B2018-07-03%2Bat%2B8.29.21%2BAM.png" width="640" height="136" border="0" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Add responses, too (if your path needs them).&lt;/p&gt; &lt;p&gt;&lt;a href="http://1.bp.blogspot.com/-QmmJKkRFnqg/WzuCYXCyjoI/AAAAAAAAFno/bw8o469bwoMuZ07ey1GyZs1r1XGzJEsZQCK4BGAYYCw/s1600/Screen%2BShot%2B2018-07-03%2Bat%2B8.36.37%2BAM.png"&gt;&lt;img src="https://1.bp.blogspot.com/-QmmJKkRFnqg/WzuCYXCyjoI/AAAAAAAAFno/bw8o469bwoMuZ07ey1GyZs1r1XGzJEsZQCK4BGAYYCw/s640/Screen%2BShot%2B2018-07-03%2Bat%2B8.36.37%2BAM.png" width="640" height="408" border="0" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Once you are done, it&amp;#8217;s time to export the API standard document.&lt;/p&gt; &lt;p&gt;&lt;a href="http://4.bp.blogspot.com/-XDM2UyK6Bsk/WzuDBZjtgwI/AAAAAAAAFn0/Ks_HsobFs1EIXRTblCpKXGW0vthr90jhQCK4BGAYYCw/s1600/Screen%2BShot%2B2018-07-03%2Bat%2B8.43.13%2BAM.png"&gt;&lt;img src="https://4.bp.blogspot.com/-XDM2UyK6Bsk/WzuDBZjtgwI/AAAAAAAAFn0/Ks_HsobFs1EIXRTblCpKXGW0vthr90jhQCK4BGAYYCw/s640/Screen%2BShot%2B2018-07-03%2Bat%2B8.43.13%2BAM.png" width="640" height="292" border="0" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;h3&gt;&lt;b&gt;Generating the Red Hat Fuse Project from the Standard API Document&lt;/b&gt;&lt;/h3&gt; &lt;p&gt;Go to Red Hat Developer Studio and create a new Red Hat Fuse project by right-clicking in the navigation panel and selecting &lt;strong&gt;New-&amp;#62;Fuse Integration Project&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href="http://4.bp.blogspot.com/-WJPxKJf_8sQ/WzuDXnvgfNI/AAAAAAAAFoI/v5x9AxjkvO0OMc-q_fF1CeW2cBW5IWNIwCK4BGAYYCw/s1600/Screen%2BShot%2B2018-07-03%2Bat%2B10.08.18%2BAM.png"&gt;&lt;img src="https://4.bp.blogspot.com/-WJPxKJf_8sQ/WzuDXnvgfNI/AAAAAAAAFoI/v5x9AxjkvO0OMc-q_fF1CeW2cBW5IWNIwCK4BGAYYCw/s640/Screen%2BShot%2B2018-07-03%2Bat%2B10.08.18%2BAM.png" width="640" height="224" border="0" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Provide a name for the project.&lt;/p&gt; &lt;div&gt;&lt;a href="http://1.bp.blogspot.com/-H3aeymLiXIk/WzuDQ6MwfrI/AAAAAAAAFn8/W0u6VQx5AWg8_NPsVMYNwpeh5xZ72NEvQCK4BGAYYCw/s1600/Screen%2BShot%2B2018-07-03%2Bat%2B8.51.36%2BAM.png"&gt;&lt;img src="https://1.bp.blogspot.com/-H3aeymLiXIk/WzuDQ6MwfrI/AAAAAAAAFn8/W0u6VQx5AWg8_NPsVMYNwpeh5xZ72NEvQCK4BGAYYCw/s320/Screen%2BShot%2B2018-07-03%2Bat%2B8.51.36%2BAM.png" width="320" height="212" border="0" /&gt;&lt;/a&gt;&lt;/div&gt; &lt;p&gt;We are going to use a microservices approach, so select the most-popular runtime—Spring Boot. We will be running this on the Red Hat OpenShift cloud platform.&lt;/p&gt; &lt;div&gt;&lt;a href="http://4.bp.blogspot.com/-FG9ynb0VjVg/WzuEF0ADebI/AAAAAAAAFoY/si2x1omk0pwJFHXz2z39ipS3FHMMV1EbwCK4BGAYYCw/s1600/Screen%2BShot%2B2018-07-03%2Bat%2B8.51.45%2BAM.png"&gt;&lt;img src="https://4.bp.blogspot.com/-FG9ynb0VjVg/WzuEF0ADebI/AAAAAAAAFoY/si2x1omk0pwJFHXz2z39ipS3FHMMV1EbwCK4BGAYYCw/s400/Screen%2BShot%2B2018-07-03%2Bat%2B8.51.45%2BAM.png" width="356" height="400" border="0" /&gt;&lt;/a&gt;&lt;/div&gt; &lt;p&gt;Select the Spring DSL template.&lt;/p&gt; &lt;div&gt;&lt;a href="http://1.bp.blogspot.com/-wy1LtMUGz7Y/WzuEaVaCtWI/AAAAAAAAFok/Y6g8jJ3aJEs2rq7uifgEyKOPnxC6N3c5QCK4BGAYYCw/s1600/Screen%2BShot%2B2018-07-03%2Bat%2B8.51.59%2BAM.png"&gt;&lt;img src="https://1.bp.blogspot.com/-wy1LtMUGz7Y/WzuEaVaCtWI/AAAAAAAAFok/Y6g8jJ3aJEs2rq7uifgEyKOPnxC6N3c5QCK4BGAYYCw/s400/Screen%2BShot%2B2018-07-03%2Bat%2B8.51.59%2BAM.png" width="360" height="400" border="0" /&gt;&lt;/a&gt;&lt;/div&gt; &lt;p&gt;You will then have a sample Red Hat Fuse project:&lt;/p&gt; &lt;p&gt;&lt;a href="http://2.bp.blogspot.com/-8yKi5i3kJdw/WzuFrH9Y0lI/AAAAAAAAFow/AVHm18CKbkIELG0mL3uSLCnzJOMF_EsgQCK4BGAYYCw/s1600/Screen%2BShot%2B2018-07-03%2Bat%2B10.17.38%2BAM.png"&gt;&lt;img src="https://2.bp.blogspot.com/-8yKi5i3kJdw/WzuFrH9Y0lI/AAAAAAAAFow/AVHm18CKbkIELG0mL3uSLCnzJOMF_EsgQCK4BGAYYCw/s640/Screen%2BShot%2B2018-07-03%2Bat%2B10.17.38%2BAM.png" width="640" height="144" border="0" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Add the generated API specification document to the directory &lt;code&gt;src/spec/&lt;/code&gt;.&lt;/p&gt; &lt;div class="separator"&gt;&lt;a href="http://3.bp.blogspot.com/-gNgkWcdI64c/WzuNhvUnKmI/AAAAAAAAFo8/kmW2PwCHAg8sjlnNwQvvKgbxZ7dsiUqMACK4BGAYYCw/s1600/Screen%2BShot%2B2018-07-03%2Bat%2B10.50.43%2BAM.png"&gt;&lt;img src="https://3.bp.blogspot.com/-gNgkWcdI64c/WzuNhvUnKmI/AAAAAAAAFo8/kmW2PwCHAg8sjlnNwQvvKgbxZ7dsiUqMACK4BGAYYCw/s320/Screen%2BShot%2B2018-07-03%2Bat%2B10.50.43%2BAM.png" width="320" height="277" border="0" /&gt;&lt;/a&gt;&lt;/div&gt; &lt;p&gt;Edit the &lt;code&gt;pom.xml&lt;/code&gt; file, and add the following to it:&lt;/p&gt; &lt;pre&gt;&amp;#60;plugins&amp;#62; .... &amp;#60;plugin&amp;#62;   &amp;#60;groupId&amp;#62;org.apache.camel&amp;#60;/groupId&amp;#62;   &amp;#60;artifactId&amp;#62;camel-restdsl-swagger-plugin&amp;#60;/artifactId&amp;#62;   &amp;#60;version&amp;#62;2.21.0&amp;#60;/version&amp;#62;   &amp;#60;configuration&amp;#62;     &amp;#60;specificationUri&amp;#62;src/spec/MyCustomer.json&amp;#60;/specificationUri&amp;#62;     &amp;#60;fileName&amp;#62;camel-rest.xml&amp;#60;/fileName&amp;#62;     &amp;#60;outputDirectory&amp;#62;src/main/resources/spring&amp;#60;/outputDirectory&amp;#62;       &amp;#60;/configuration&amp;#62; &amp;#60;/plugin&amp;#62; .... &amp;#60;/plugin&amp;#62; &lt;/pre&gt; &lt;p&gt;Generate the XML by running the following in the command-line tool: &lt;b&gt;  &lt;/b&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;mvn camel-restdsl-swagger:generate-xml&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="http://3.bp.blogspot.com/-aqD9wgB3KFE/WzuW2nHTU1I/AAAAAAAAFpU/HOO-xkHd89QeWNkBD2kaBjuLMbUCv9ZRQCK4BGAYYCw/s1600/Screen%2BShot%2B2018-07-03%2Bat%2B11.31.08%2BAM.png"&gt;&lt;img src="https://3.bp.blogspot.com/-aqD9wgB3KFE/WzuW2nHTU1I/AAAAAAAAFpU/HOO-xkHd89QeWNkBD2kaBjuLMbUCv9ZRQCK4BGAYYCw/s640/Screen%2BShot%2B2018-07-03%2Bat%2B11.31.08%2BAM.png" width="640" height="190" border="0" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You will then find a newly generated Camel context named &lt;code&gt;camel-rest.xml&lt;/code&gt;, which has all the path implementations in Camel.&lt;/p&gt; &lt;p&gt;&lt;a href="http://2.bp.blogspot.com/-ZHrnbN0ukGs/WzuYTa0XpxI/AAAAAAAAFps/sAGdtJQUmDgbyJ4Nb8Zzp30F6UJ5jFcpACK4BGAYYCw/s1600/Screen%2BShot%2B2018-07-03%2Bat%2B11.33.36%2BAM.png"&gt;&lt;img src="https://2.bp.blogspot.com/-ZHrnbN0ukGs/WzuYTa0XpxI/AAAAAAAAFps/sAGdtJQUmDgbyJ4Nb8Zzp30F6UJ5jFcpACK4BGAYYCw/s640/Screen%2BShot%2B2018-07-03%2Bat%2B11.33.36%2BAM.png" width="640" height="206" border="0" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;#160;&lt;/p&gt; &lt;p&gt;&amp;#160;&lt;/p&gt; &lt;p&gt;From that file, copy everything inside the &lt;code&gt;&amp;#60;rests&amp;#62;&lt;/code&gt; tags and paste it into the original &lt;code&gt;camel-context.xml&lt;/code&gt; file inside &lt;code&gt;camelContext&lt;/code&gt;. Add the following rest configuration on top of the rest block.&lt;/p&gt; &lt;pre&gt;&amp;#60;restConfiguration apiContextPath="api-docs" bindingMode="auto"             component="undertow" contextPath="/customer"             enableCORS="true" port="8080"&amp;#62;    &amp;#60;apiProperty key="cors" value="true"/&amp;#62;    &amp;#60;apiProperty key="api.title" value="Customer Service"/&amp;#62;    &amp;#60;apiProperty key="api.version" value="1.0.0"/&amp;#62; &amp;#60;/restConfiguration&amp;#62;&lt;/pre&gt; &lt;div&gt;&lt;/div&gt; &lt;div&gt;&lt;/div&gt; &lt;div&gt;&lt;/div&gt; &lt;div&gt;&lt;a href="http://3.bp.blogspot.com/-TcwfnhhYf_Y/WzufhZBQE5I/AAAAAAAAFp4/gJ25mM5u_5QW1qKOpiWQddOuh1daxuVIwCK4BGAYYCw/s1600/Screen%2BShot%2B2018-07-03%2Bat%2B12.07.58%2BPM.png"&gt;&lt;img src="https://3.bp.blogspot.com/-TcwfnhhYf_Y/WzufhZBQE5I/AAAAAAAAFp4/gJ25mM5u_5QW1qKOpiWQddOuh1daxuVIwCK4BGAYYCw/s640/Screen%2BShot%2B2018-07-03%2Bat%2B12.07.58%2BPM.png" width="640" height="222" border="0" /&gt;&lt;/a&gt;&lt;/div&gt; &lt;div&gt;&lt;/div&gt; &lt;div&gt;&lt;/div&gt; &lt;div&gt;&lt;/div&gt; &lt;div&gt;&lt;/div&gt; &lt;div&gt;Delete the generated &lt;code&gt;camel-rest.xml&lt;/code&gt; file.&lt;/div&gt; &lt;h3&gt;Mocking the APIs with Apache Camel&lt;/h3&gt; &lt;p&gt;We will mock the returned result by adding a constant, defined bean in the Camel context.&lt;/p&gt; &lt;p&gt;To do that, in the &lt;code&gt;src/main.resource/spring&lt;/code&gt; folder, add a &lt;code&gt;beans.xml&lt;/code&gt;&lt;b&gt; &lt;/b&gt;file&lt;b&gt; &lt;/b&gt;by right-clicking the folder and selecting &lt;strong&gt;New-&amp;#62;beans.xml File&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href="http://2.bp.blogspot.com/-Og8GmwF5Ri8/Wz4sotd7_nI/AAAAAAAAFqI/ZJA_QTS96-0QuhKo0iLClYb4YdL87On7wCK4BGAYYCw/s1600/Screen%2BShot%2B2018-07-05%2Bat%2B10.34.50%2BAM.png"&gt;&lt;img src="https://2.bp.blogspot.com/-Og8GmwF5Ri8/Wz4sotd7_nI/AAAAAAAAFqI/ZJA_QTS96-0QuhKo0iLClYb4YdL87On7wCK4BGAYYCw/s640/Screen%2BShot%2B2018-07-05%2Bat%2B10.34.50%2BAM.png" width="640" height="346" border="0" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Insert the following code snippet to the &lt;code&gt;beans.xml&lt;/code&gt; file:&lt;/p&gt; &lt;pre&gt;&amp;#60;util:list id="CustomerList" list-class="java.util.ArrayList"&amp;#62;    &amp;#60;ref bean="Customer"/&amp;#62; &amp;#60;/util:list&amp;#62; &amp;#60;util:map id="Customer" map-class="java.util.HashMap"&amp;#62;    &amp;#60;entry key="name" value="Christina"/&amp;#62;    &amp;#60;entry key="age" value="28"/&amp;#62;    &amp;#60;entry key="contact" value="765483921"/&amp;#62; &amp;#60;/util:map&amp;#62; &lt;/pre&gt; &lt;p&gt;Add the Camel routes to the &lt;code&gt;camel-context.xml&lt;/code&gt; file. The first one returns mock customer data, and the second one takes customer info as input.&lt;/p&gt; &lt;pre&gt;&amp;#60;route id="rest1-route"&amp;#62;  &amp;#60;from id="restone" uri="direct:rest1"/&amp;#62;   &amp;#60;setBody id="route-setBody1"&amp;#62;     &amp;#60;simple&amp;#62;bean:CustomerList?method=get(0)&amp;#60;/simple&amp;#62;   &amp;#60;/setBody&amp;#62; &amp;#60;/route&amp;#62; &amp;#60;route id="rest2-route"&amp;#62;   &amp;#60;from id="resttwo" uri="direct:rest2"/&amp;#62;   &amp;#60;log id="input-log" message="&amp;#62;&amp;#62;&amp;#62; ${body}"/&amp;#62;     &amp;#60;setBody id="route-setBody2"&amp;#62;       &amp;#60;simple&amp;#62;Customer created&amp;#60;/simple&amp;#62;     &amp;#60;/setBody&amp;#62; &amp;#60;/route&amp;#62;&lt;/pre&gt; &lt;p&gt;Now, it&amp;#8217;s time to set up the dependency libraries in the &lt;code&gt;pom.xml&lt;/code&gt; file:&lt;/p&gt; &lt;pre&gt;&amp;#60;dependency&amp;#62;   &amp;#60;groupId&amp;#62;org.springframework.boot&amp;#60;/groupId&amp;#62;   &amp;#60;artifactId&amp;#62;spring-boot-starter-undertow&amp;#60;/artifactId&amp;#62; &amp;#60;/dependency&amp;#62; &amp;#60;dependency&amp;#62;   &amp;#60;groupId&amp;#62;org.apache.camel&amp;#60;/groupId&amp;#62;   &amp;#60;artifactId&amp;#62;camel-undertow-starter&amp;#60;/artifactId&amp;#62; &amp;#60;/dependency&amp;#62;  &amp;#60;dependency&amp;#62;   &amp;#60;groupId&amp;#62;org.apache.camel&amp;#60;/groupId&amp;#62;   &amp;#60;artifactId&amp;#62;camel-jackson-starter&amp;#60;/artifactId&amp;#62; &amp;#60;/dependency&amp;#62;  &amp;#60;dependency&amp;#62;   &amp;#60;groupId&amp;#62;org.apache.camel&amp;#60;/groupId&amp;#62;   &amp;#60;artifactId&amp;#62;camel-swagger-java-starter&amp;#60;/artifactId&amp;#62; &amp;#60;/dependency&amp;#62; &lt;/pre&gt; &lt;div&gt;Finally, it&amp;#8217;s time to test by running the following at the command line:&lt;/div&gt; &lt;p&gt;&lt;code&gt;mvn sprint-boot:run&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Two endpoints will be exposed for testing:&lt;/p&gt; &lt;pre&gt;Christina Laptop$ curl http://YOURIP:8080/customer/id/123 {"name":"Christina","age":"28","contact":"765483921"} Christina Laptop$ curl --header "Content-Type: application/json"   --request PUT   --data '{"name":"Christina","age":28,"contact":"765483921"}'   http://YOURIP:8080/customer/add "Customer created" &lt;/pre&gt; &lt;p&gt;You are now ready for the consumer to start testing the APIs.&lt;/p&gt; &lt;p&gt;In the next article in this series, I will take you through how to actually implement the API, expose it in the cloud, and manage it.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;For more information see these Red Hat Developer resources:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;[DevNation Live] &amp;#8211; &lt;a href="https://developers.redhat.com/videos/youtube/zuEYtMvHN6g/"&gt;Camel Riders in the Cloud&lt;/a&gt; &amp;#8211; Watch the recording of Claus Ibsen,  author of the Camel in Action books.&lt;/li&gt; &lt;li&gt;Download the free ebook &amp;#8211; &lt;a href="https://developers.redhat.com/books/selections-camel-action/"&gt;Selections from Camel in Action, 2nd edition&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Learn more about &lt;a href="https://developers.redhat.com/products/fuse/overview/"&gt;Red Hat Fuse&lt;/a&gt; &amp;#8211; a distributed, cloud-native integration solution that is based on Camel&lt;/li&gt; &lt;li&gt;Read &lt;a href="https://developers.redhat.com/blog/2018/04/11/api-journey-idea-deployment-agile-part1/"&gt;An API Journey: From idea to deployment the Agile Way &amp;#8211; Part 1&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F07%2F12%2Fcontract-first-api-design-with-apicurio-and-red-hat-fuse%2F&amp;#38;linkname=Contract-First%20API%20Design%20with%20Apicurio%20and%20Red%20Hat%20Fuse%2FCamel" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F07%2F12%2Fcontract-first-api-design-with-apicurio-and-red-hat-fuse%2F&amp;#38;linkname=Contract-First%20API%20Design%20with%20Apicurio%20and%20Red%20Hat%20Fuse%2FCamel" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_google_plus" href="https://www.addtoany.com/add_to/google_plus?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F07%2F12%2Fcontract-first-api-design-with-apicurio-and-red-hat-fuse%2F&amp;#38;linkname=Contract-First%20API%20Design%20with%20Apicurio%20and%20Red%20Hat%20Fuse%2FCamel" title="Google+" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F07%2F12%2Fcontract-first-api-design-with-apicurio-and-red-hat-fuse%2F&amp;#38;linkname=Contract-First%20API%20Design%20with%20Apicurio%20and%20Red%20Hat%20Fuse%2FCamel" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F07%2F12%2Fcontract-first-api-design-with-apicurio-and-red-hat-fuse%2F&amp;#38;linkname=Contract-First%20API%20Design%20with%20Apicurio%20and%20Red%20Hat%20Fuse%2FCamel" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F07%2F12%2Fcontract-first-api-design-with-apicurio-and-red-hat-fuse%2F&amp;#38;linkname=Contract-First%20API%20Design%20with%20Apicurio%20and%20Red%20Hat%20Fuse%2FCamel" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F07%2F12%2Fcontract-first-api-design-with-apicurio-and-red-hat-fuse%2F&amp;#38;linkname=Contract-First%20API%20Design%20with%20Apicurio%20and%20Red%20Hat%20Fuse%2FCamel" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F07%2F12%2Fcontract-first-api-design-with-apicurio-and-red-hat-fuse%2F&amp;#38;linkname=Contract-First%20API%20Design%20with%20Apicurio%20and%20Red%20Hat%20Fuse%2FCamel" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F07%2F12%2Fcontract-first-api-design-with-apicurio-and-red-hat-fuse%2F&amp;#38;title=Contract-First%20API%20Design%20with%20Apicurio%20and%20Red%20Hat%20Fuse%2FCamel" data-a2a-url="https://developers.redhat.com/blog/2018/07/12/contract-first-api-design-with-apicurio-and-red-hat-fuse/" data-a2a-title="Contract-First API Design with Apicurio and Red Hat Fuse/Camel"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2018/07/12/contract-first-api-design-with-apicurio-and-red-hat-fuse/"&gt;Contract-First API Design with Apicurio and Red Hat Fuse/Camel&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;RHD Blog&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/C5nm9tnzf08" height="1" width="1" alt=""/&gt;</content><summary>This is part one of my two-article series that demonstrates how to implement contract-first API design using Apicurio and Red Hat Fuse.  It covers how to create an OpenAPI standard document as the contract between API providers and consumers using Apicurio Studio. It also shows how to quickly create mock tests using Red Hat Fuse which is based on Camel. There are two common approaches when it come...</summary><dc:creator>Christina Lin</dc:creator><dc:date>2018-07-12T16:00:52Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2018/07/12/contract-first-api-design-with-apicurio-and-red-hat-fuse/</feedburner:origLink></entry><entry><title>Protect Wildfly HTTP Management Interface with SSL</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/1v4GEvUWrqU/" /><category term="feed_group_name_wildfly" scheme="searchisko:content:tags" /><category term="feed_name_hal" scheme="searchisko:content:tags" /><author><name>Harald Pehl</name></author><id>searchisko:content:id:jbossorg_blog-protect_wildfly_http_management_interface_with_ssl</id><updated>2018-07-12T13:30:25Z</updated><published>2018-07-12T13:30:25Z</published><content type="html">In WildFly 13, there is a wizard to enable the SSL for the HTTP management interface, it uses the elytron resource to manage the security features. The certificate may be handled in the following ways: Generate a self-signed certificate Use an existing certificate file Use an existing elytron key-store Use mutual authentication with a client You can see more information about elytron features and HTTPS in general.&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/1v4GEvUWrqU" height="1" width="1" alt=""/&gt;</content><summary>In WildFly 13, there is a wizard to enable the SSL for the HTTP management interface, it uses the elytron resource to manage the security features. The certificate may be handled in the following ways: Generate a self-signed certificate Use an existing certificate file Use an existing elytron key-store Use mutual authentication with a client You can see more information about elytron features and ...</summary><dc:creator>Harald Pehl</dc:creator><dc:date>2018-07-12T13:30:25Z</dc:date><feedburner:origLink>https://hal.github.io/blog/protect-mgmt-interface-ssl/</feedburner:origLink></entry><entry><title>Debezium 0.8 Final Is Released</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/j2kHXWMDzj0/" /><category term="docker" scheme="searchisko:content:tags" /><category term="feed_group_name_debezium" scheme="searchisko:content:tags" /><category term="feed_name_debezium" scheme="searchisko:content:tags" /><category term="mongodb" scheme="searchisko:content:tags" /><category term="mysql" scheme="searchisko:content:tags" /><category term="oracle" scheme="searchisko:content:tags" /><category term="postgres" scheme="searchisko:content:tags" /><category term="releases" scheme="searchisko:content:tags" /><author><name>Gunnar Morling</name></author><id>searchisko:content:id:jbossorg_blog-debezium_0_8_final_is_released</id><updated>2018-07-13T13:43:36Z</updated><published>2018-07-12T00:00:00Z</published><content type="html">&lt;div id="preamble"&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;I’m very happy to announce the release of Debezium &lt;strong&gt;0.8.0.Final&lt;/strong&gt;!&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The key features of Debezium 0.8 are the first work-in-progress version of our &lt;a href="http://debezium.io/docs/connectors/oracle/"&gt;Oracle connector&lt;/a&gt; (based on the XStream API) and a brand-new parser for MySQL DDL statements. Besides that, there are plenty of smaller new features (e.g. propagation of default values to corresponding Connect schemas, optional propagation of source queries in CDC messages and a largely improved SMT for sinking changes from MongoDB into RDBMS) as well as lots of bug fixes (e.g. around temporal and numeric column types, large transactions with Postgres).&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Please see the previous announcements (&lt;a href="http://debezium.io/blog/2018/06/21/debezium-0-8-0-beta1-released/"&gt;Beta 1&lt;/a&gt;, &lt;a href="http://debezium.io/blog/2018/07/04/debezium-0-8-0-cr1-released/"&gt;CR 1&lt;/a&gt;) to learn about all the changes in more depth. The Final release largely resembles CR1; apart from further improvements to the Oracle connector (&lt;a href="https://issues.jboss.org/browse/DBZ-762"&gt;DBZ-792&lt;/a&gt;) there’s one nice addition to the MySQL connector contributed by &lt;a href="https://github.com/pgoranss"&gt;Peter Goransson&lt;/a&gt;: when doing a snapshot, it will now expose information about the processed rows via JMX (&lt;a href="https://issues.jboss.org/browse/DBZ-789"&gt;DBZ-789&lt;/a&gt;), which is very handy when snapshotting larger tables.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Please take a look at the &lt;a href="http://debezium.io/docs/releases/#release-0-8-0-final"&gt;change log&lt;/a&gt; for the complete list of changes in 0.8.0.Final and general upgrade notes.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="what_s_next"&gt;&lt;a class="anchor" href="#what_s_next"&gt;&lt;/a&gt;What’s next?&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;We’re continuing our work on the Oracle connector. The work on initial snapshotting is well progressing and it should be part of the next release. Other improvements will be support for structural changes to captured tables after the initial snapshot has been made, more extensive source info metadata and more. Please track &lt;a href="https://issues.jboss.org/browse/DBZ-716"&gt;DBZ-716&lt;/a&gt; for this work; the improvements are planned to be released incrementally in the upcoming versions of Debezium.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;We’ve also started to explore ingesting changes via LogMiner. This is more involved in terms of engineering efforts than using XStream, but it comes with the huge advantage of not requiring a separate license (LogMiner comes with the Oracle database itself). It’s not quite clear yet when we can release something on this front, and we’re also actively exploring further alternatives. But we are quite optimistic and hope to have something some time soon.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The other focus of work is a connector for SQL Server (see &lt;a href="https://issues.jboss.org/browse/DBZ-40"&gt;DBZ-40&lt;/a&gt;). Work on this has started as well, and there should be an Alpha1 release of Debezium 0.9 with a first drop of that connector within the next few weeks.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;To find out about some more long term ideas, please check out our &lt;a href="http://debezium.io/docs/roadmap/"&gt;roadmap&lt;/a&gt; and get in touch with us, if you got any ideas or suggestions for future development.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="about_debezium"&gt;&lt;a class="anchor" href="#about_debezium"&gt;&lt;/a&gt;About Debezium&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of &lt;a href="http://kafka.apache.org/"&gt;Kafka&lt;/a&gt; and provides &lt;a href="http://kafka.apache.org/documentation.html#connect"&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is &lt;a href="http://debezium.io/license"&gt;open source&lt;/a&gt; under the &lt;a href="http://www.apache.org/licenses/LICENSE-2.0.html"&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="get_involved"&gt;&lt;a class="anchor" href="#get_involved"&gt;&lt;/a&gt;Get involved&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter &lt;a href="https://twitter.com/debezium"&gt;@debezium&lt;/a&gt;, &lt;a href="https://gitter.im/debezium/user"&gt;chat with us on Gitter&lt;/a&gt;, or join our &lt;a href="https://groups.google.com/forum/#!forum/debezium"&gt;mailing list&lt;/a&gt; to talk with the community. All of the code is open source &lt;a href="https://github.com/debezium/"&gt;on GitHub&lt;/a&gt;, so build the code locally and help us improve ours existing connectors and add even more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href="https://issues.jboss.org/projects/DBZ/issues/"&gt;log an issue&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/j2kHXWMDzj0" height="1" width="1" alt=""/&gt;</content><summary>I’m very happy to announce the release of Debezium 0.8.0.Final! The key features of Debezium 0.8 are the first work-in-progress version of our Oracle connector (based on the XStream API) and a brand-new parser for MySQL DDL statements. Besides that, there are plenty of smaller new features (e.g. propagation of default values to corresponding Connect schemas, optional propagation of source queries ...</summary><dc:creator>Gunnar Morling</dc:creator><dc:date>2018-07-12T00:00:00Z</dc:date><feedburner:origLink>http://debezium.io/blog/2018/07/12/debezium-0-8-0-final-released/</feedburner:origLink></entry><entry><title>PodCTL Podcast #38 – A Beginner’s Guide to Kubernetes</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/hs5snq0_zTo/" /><category term="Containers" scheme="searchisko:content:tags" /><category term="docker" scheme="searchisko:content:tags" /><category term="feed_group_name_nonmiddleware" scheme="searchisko:content:tags" /><category term="feed_name_redhat_developer_blog" scheme="searchisko:content:tags" /><category term="Kubernetes" scheme="searchisko:content:tags" /><category term="podcasts" scheme="searchisko:content:tags" /><category term="PodCTL Podcast" scheme="searchisko:content:tags" /><category term="prometheus" scheme="searchisko:content:tags" /><category term="Red Hat OpenShift" scheme="searchisko:content:tags" /><category term="Red Hat OpenShift Container Platform" scheme="searchisko:content:tags" /><author><name>Rob Terzi</name></author><id>searchisko:content:id:jbossorg_blog-podctl_podcast_38_a_beginner_s_guide_to_kubernetes</id><updated>2018-07-11T11:00:43Z</updated><published>2018-07-11T11:00:43Z</published><content type="html">&lt;p&gt;If you aren&amp;#8217;t following the &lt;a href="https://blog.openshift.com/"&gt;OpenShift Blog&lt;/a&gt;, you might not be aware of the &lt;a href="https://twitter.com/PodCtl"&gt;PodCTL&lt;/a&gt; podcast. It&amp;#8217;s a free weekly tech podcast covering containers, kubernetes, and OpenShift hosted by Red Hat&amp;#8217;s Brian Gracely (&lt;a href="https://twitter.com/bgracely"&gt;@bgracely&lt;/a&gt;) and Tyler Britten (&lt;a href="https://twitter.com/vmtyler"&gt;@vmtyler&lt;/a&gt;). I&amp;#8217;m reposting this episode here on the Red Hat Developer Blog because I think their realization is spot on—while early adopters might be deep into Kubernetes, many are just starting and could benefit from some insights.&lt;/p&gt; &lt;h3&gt;&lt;em&gt;Original Introduction from &lt;a href="https://blog.openshift.com/podcast-podctl-38-a-beginners-guide-to-kubernetes/"&gt;blog.openshift.com:&lt;/a&gt;&lt;/em&gt;&lt;/h3&gt; &lt;p&gt;The Kubernetes community now has 10 releases (2.5 yrs) of software and experience. We just finished &lt;a href="https://www.youtube.com/playlist?list=PLj6h78yzYM2N8GdbjmhVU65KYm_68qBmo"&gt;KubeCon Copenhagen&lt;/a&gt;, &lt;a href="https://www.youtube.com/playlist?list=PLaR6Rq6Z4Iqe5zqMhgvStXJB9d83rKRHf"&gt;OpenShift Commons Gathering&lt;/a&gt;, and &lt;a href="https://www.youtube.com/playlist?list=PLaR6Rq6Z4IqdtOkaBwLBg2kjmFa8I0hwJ"&gt;Red Hat Summit&lt;/a&gt; and we heard lots of companies talk about their deployments and journeys. But many of them took a while (12–18) months to get to where they are today. This feels like the “early adopters” and we’re beginning to get to the “crossing the chasm” part of the market. So thought we’d discuss some of the basics, lessons learned, and other things people could use to “fast-track” what they need to be successful with Kubernetes.&lt;/p&gt; &lt;p&gt;The podcast will always be available on the Red Hat OpenShift blog (&lt;a href="https://blog.openshift.com/search/?refinement=blog&amp;#38;query=PodCTL"&gt;search: #PodCTL&lt;/a&gt;), as well as on &lt;a href="http://bit.ly/2uWqaHe"&gt;RSS Feeds&lt;/a&gt;, &lt;a href="https://itunes.apple.com/us/podcast/podctl-1-3-6-ways-to-love-kubernetes/id1270983443?i=1000390948443&amp;#38;mt=2"&gt;iTunes&lt;/a&gt;, &lt;a href="http://bit.ly/2uIGoo5"&gt;Google Play&lt;/a&gt;, &lt;a href="http://bit.ly/2vWmZnG"&gt;Stitcher&lt;/a&gt;, &lt;a href="https://tunein.com/radio/PodCTL---Containers--Kubernetes--OpenShift-p1024049/"&gt;TuneIn&lt;/a&gt;, and all your favorite podcast players.&lt;/p&gt; &lt;p&gt;&lt;span id="more-505767"&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2&gt;PodCTL #38 – A Beginners Guide to Kubernetes&lt;/h2&gt; &lt;p&gt;June 04, 2018&lt;br /&gt; Brian Gracely &amp;#38; Tyler Britten&lt;br /&gt; PodCTL &amp;#8211; Containers | Kubernetes | OpenShift&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.buzzsprout.com/110399/718643-podctl-38-a-beginners-guide-to-kubernetes"&gt;&lt;img class=" alignnone size-medium wp-image-505927 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2018/07/play-podcast.jpg" src="https://developers.redhat.com/blog/wp-content/uploads/2018/07/play-podcast-300x33.jpg" alt="" width="300" height="33" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/07/play-podcast-300x33.jpg 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/07/play-podcast.jpg 700w" sizes="(max-width: 300px) 100vw, 300px" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;#160;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Overview&lt;/strong&gt;: Brian and Tyler talk some of the basics, lessons learned, and other things people could use to “fast-track” what they need to be successful with Kubernetes.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Show Notes:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.katacoda.com/courses/kubernetes"&gt;Learn Kubernetes&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://learn.openshift.com/"&gt;Learn OpenShift&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.katacoda.com/courses/docker"&gt;Learn Containers (with Docker)&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.katacoda.com/courses/containers-without-docker"&gt;Learn Containers (without Docker)&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.katacoda.com/courses/prometheus"&gt;Learn Prometheus&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Show Premise:&lt;/strong&gt;&lt;br /&gt; The Kubernetes community now has 10 releases (2.5 yrs) of software and experience. We just finished KubeCon and Red Hat Summit and we heard lots of companies talk about their deployments and journeys. But many of them took a while (12–18) months to get to where they are today. This feels like the “early adopters” and we’re beginning to get to the “crossing the chasm” part of the market. So thought we’d discuss some of the basics, lessons learned, and other things people could use to “fast-track” what they need to be successful with Kubernetes.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Topic 1:&lt;/strong&gt; What are the core skills needed for a team that manages/runs/interacts with a Kubernetes environment?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ops Skills&lt;/li&gt; &lt;li&gt;Dev Skills&lt;/li&gt; &lt;li&gt;Compliance Skills / Security Skills&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Topic 2:&lt;/strong&gt; What has significantly changed in the Kubernetes world since 2015/16 to today that people should consider taking advantage of?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Persistence&lt;/li&gt; &lt;li&gt;Immutability&lt;/li&gt; &lt;li&gt;Operators&lt;/li&gt; &lt;li&gt;Native tools vs. Config Mgmt tools&lt;/li&gt; &lt;li&gt;Storage&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Topic 3:&lt;/strong&gt; What do you consider “still hard” and should probably justify more early effort?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Security?&lt;/li&gt; &lt;li&gt;Storage?&lt;/li&gt; &lt;li&gt;Monitoring?&lt;/li&gt; &lt;li&gt;Being overly precise about capacity planning?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Topic 4:&lt;/strong&gt; What patterns have you seen from successful deployments and customer behaviors?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Feedback?&lt;/strong&gt;&lt;br /&gt; Email: &lt;a href="mailto:podctl@gmail.com"&gt;PodCTL at gmail dot com&lt;/a&gt;&lt;br /&gt; Twitter: &lt;a href="https://twitter.com/PodCTL"&gt;@PodCTL&lt;/a&gt;&lt;br /&gt; Web: &lt;a href="https://blog.openshift.com/search/?refinement=blog&amp;#38;amp;query=PodCTL"&gt;http://blog.openshift.com&lt;/a&gt;, search #PodCTL&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F07%2F11%2Fpodcast-38-a-beginners-guide-to-kubernetes%2F&amp;#38;linkname=PodCTL%20Podcast%20%2338%20%E2%80%93%20A%20Beginner%E2%80%99s%20Guide%20to%20Kubernetes" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F07%2F11%2Fpodcast-38-a-beginners-guide-to-kubernetes%2F&amp;#38;linkname=PodCTL%20Podcast%20%2338%20%E2%80%93%20A%20Beginner%E2%80%99s%20Guide%20to%20Kubernetes" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_google_plus" href="https://www.addtoany.com/add_to/google_plus?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F07%2F11%2Fpodcast-38-a-beginners-guide-to-kubernetes%2F&amp;#38;linkname=PodCTL%20Podcast%20%2338%20%E2%80%93%20A%20Beginner%E2%80%99s%20Guide%20to%20Kubernetes" title="Google+" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F07%2F11%2Fpodcast-38-a-beginners-guide-to-kubernetes%2F&amp;#38;linkname=PodCTL%20Podcast%20%2338%20%E2%80%93%20A%20Beginner%E2%80%99s%20Guide%20to%20Kubernetes" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F07%2F11%2Fpodcast-38-a-beginners-guide-to-kubernetes%2F&amp;#38;linkname=PodCTL%20Podcast%20%2338%20%E2%80%93%20A%20Beginner%E2%80%99s%20Guide%20to%20Kubernetes" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F07%2F11%2Fpodcast-38-a-beginners-guide-to-kubernetes%2F&amp;#38;linkname=PodCTL%20Podcast%20%2338%20%E2%80%93%20A%20Beginner%E2%80%99s%20Guide%20to%20Kubernetes" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F07%2F11%2Fpodcast-38-a-beginners-guide-to-kubernetes%2F&amp;#38;linkname=PodCTL%20Podcast%20%2338%20%E2%80%93%20A%20Beginner%E2%80%99s%20Guide%20to%20Kubernetes" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F07%2F11%2Fpodcast-38-a-beginners-guide-to-kubernetes%2F&amp;#38;linkname=PodCTL%20Podcast%20%2338%20%E2%80%93%20A%20Beginner%E2%80%99s%20Guide%20to%20Kubernetes" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F07%2F11%2Fpodcast-38-a-beginners-guide-to-kubernetes%2F&amp;#38;title=PodCTL%20Podcast%20%2338%20%E2%80%93%20A%20Beginner%E2%80%99s%20Guide%20to%20Kubernetes" data-a2a-url="https://developers.redhat.com/blog/2018/07/11/podcast-38-a-beginners-guide-to-kubernetes/" data-a2a-title="PodCTL Podcast #38 – A Beginner’s Guide to Kubernetes"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2018/07/11/podcast-38-a-beginners-guide-to-kubernetes/"&gt;PodCTL Podcast #38 – A Beginner’s Guide to Kubernetes&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;RHD Blog&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/hs5snq0_zTo" height="1" width="1" alt=""/&gt;</content><summary>If you aren’t following the OpenShift Blog, you might not be aware of the PodCTL podcast. It’s a free weekly tech podcast covering containers, kubernetes, and OpenShift hosted by Red Hat’s Brian Gracely (@bgracely) and Tyler Britten (@vmtyler). I’m reposting this episode here on the Red Hat Developer Blog because I think their realization is spot on—while early adopters might be deep into Kubernet...</summary><dc:creator>Rob Terzi</dc:creator><dc:date>2018-07-11T11:00:43Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2018/07/11/podcast-38-a-beginners-guide-to-kubernetes/</feedburner:origLink></entry></feed>
